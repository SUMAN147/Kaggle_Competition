{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n#LSTM for sequence classification in the Imdb dataset.\nimport numpy as np\nfrom keras.datasets import imdb\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\n#Fix random seed for reproducibility\nnp.random.seed(6)","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Keras have built the vocablary of the words of the reviews and \n# sort the occurances of word in the review .\n# Load the dataset but only keep the top n words, ignore the rest\ntop_words = 5000   # it means we will use our top 5000 words for Training only\n\nold = np.load\nnp.load = lambda *a,**k: old(*a,**k,allow_pickle=True)\n(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n\nnp.load = old\ndel(old)\n\n","execution_count":2,"outputs":[{"output_type":"stream","text":"Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n17465344/17464789 [==============================] - 2s 0us/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train[1])\nprint(type(X_train[1]))\nprint(len(X_train[1]))","execution_count":3,"outputs":[{"output_type":"stream","text":"[1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 4369, 2, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 2, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2637, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 2, 5, 2, 656, 245, 2350, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n<class 'list'>\n189\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Truncate and/or pad input sequences\nmax_review_length = 600\nX_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\nX_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n\nprint(X_train.shape)\nprint(X_train[1])","execution_count":4,"outputs":[{"output_type":"stream","text":"(25000, 600)\n[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    1  194 1153  194    2   78  228    5    6\n 1463 4369    2  134   26    4  715    8  118 1634   14  394   20   13\n  119  954  189  102    5  207  110 3103   21   14   69  188    8   30\n   23    7    4  249  126   93    4  114    9 2300 1523    5  647    4\n  116    9   35    2    4  229    9  340 1322    4  118    9    4  130\n 4901   19    4 1002    5   89   29  952   46   37    4  455    9   45\n   43   38 1543 1905  398    4 1649   26    2    5  163   11 3215    2\n    4 1153    9  194  775    7    2    2  349 2637  148  605    2    2\n   15  123  125   68    2    2   15  349  165 4362   98    5    4  228\n    9   43    2 1157   15  299  120    5  120  174   11  220  175  136\n   50    9 4373  228    2    5    2  656  245 2350    5    4    2  131\n  152  491   18    2   32    2 1212   14    9    6  371   78   22  625\n   64 1382    9    8  168  145   23    4 1690   15   16    4 1355    5\n   28    6   52  154  462   33   89   78  285   16  145   95]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create the model\nembedding_vector_length = 32\nmodel = Sequential()\nmodel.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\nmodel.add(LSTM(100))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\n","execution_count":5,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 600, 32)           160000    \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 100)               53200     \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 101       \n=================================================================\nTotal params: 213,301\nTrainable params: 213,301\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train , nb_epoch=10, batch_size=64)\n#Final evaluation of the model\nscores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Accuracy: %.2f%%\" %(scores[1]*100))","execution_count":6,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n  \"\"\"Entry point for launching an IPython kernel.\n","name":"stderr"},{"output_type":"stream","text":"Epoch 1/10\n25000/25000 [==============================] - 329s 13ms/step - loss: 0.4631 - acc: 0.7797\nEpoch 2/10\n25000/25000 [==============================] - 316s 13ms/step - loss: 0.3006 - acc: 0.8774\nEpoch 3/10\n25000/25000 [==============================] - 315s 13ms/step - loss: 0.2594 - acc: 0.8989\nEpoch 4/10\n25000/25000 [==============================] - 316s 13ms/step - loss: 0.2839 - acc: 0.8931\nEpoch 5/10\n25000/25000 [==============================] - 314s 13ms/step - loss: 0.2332 - acc: 0.9128\nEpoch 6/10\n25000/25000 [==============================] - 313s 13ms/step - loss: 0.2195 - acc: 0.9160\nEpoch 7/10\n25000/25000 [==============================] - 315s 13ms/step - loss: 0.1813 - acc: 0.9323\nEpoch 8/10\n25000/25000 [==============================] - 313s 13ms/step - loss: 0.1721 - acc: 0.9358\nEpoch 9/10\n25000/25000 [==============================] - 315s 13ms/step - loss: 0.1476 - acc: 0.9458\nEpoch 10/10\n25000/25000 [==============================] - 315s 13ms/step - loss: 0.1350 - acc: 0.9498\nAccuracy: 86.34%\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}