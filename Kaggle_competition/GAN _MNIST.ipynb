{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport keras\nfrom keras.layers import Dense,Dropout, Input\nfrom keras.models import Model, Sequential\nfrom keras.datasets import mnist\nfrom tqdm import tqdm\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.optimizers import Adam","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#loading the data from mnist dataset\n\ndef load_data():\n    (x_train,y_train),(x_test,y_test) = mnist.load_data()\n    x_train = (x_train.astype(np.float32) - 127.5)/127.5             # Normalizing our data from -1 to 1 rather than 0 to 1, because GAN perform better on this.\n    \n    x_train = x_train.reshape(60000, 784)                            # Reshaping the input in (60000,784) shape\n    return (x_train, y_train, x_test , y_test)\n\n(X_train, y_train, X_test, y_test) = load_data()\nprint(X_train.shape)\n\n","execution_count":2,"outputs":[{"output_type":"stream","text":"Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n11493376/11490434 [==============================] - 2s 0us/step\n(60000, 784)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# definition of adam optimizer\ndef adam_optimizer():\n    return Adam(lr=0.0002, beta_1=0.5)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating Generator Model\ndef create_generator():\n    generator = Sequential()\n    generator.add(Dense(units=256, input_dim=100))\n    generator.add(LeakyReLU(0.2))\n    \n    generator.add(Dense(units=512))\n    generator.add(LeakyReLU(0.2))\n    \n    generator.add(Dense(units=1024))\n    generator.add(LeakyReLU(0.2))\n    \n    generator.add(Dense(units=784, activation='tanh'))\n    \n    generator.compile(loss='binary_crossentropy', optimizer=adam_optimizer())\n    return generator\n\ng = create_generator()\ng.summary()","execution_count":4,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 256)               25856     \n_________________________________________________________________\nleaky_re_lu_1 (LeakyReLU)    (None, 256)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 512)               131584    \n_________________________________________________________________\nleaky_re_lu_2 (LeakyReLU)    (None, 512)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 1024)              525312    \n_________________________________________________________________\nleaky_re_lu_3 (LeakyReLU)    (None, 1024)              0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 784)               803600    \n=================================================================\nTotal params: 1,486,352\nTrainable params: 1,486,352\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating Discriminator Model\ndef create_discriminator():\n    discriminator = Sequential()\n    discriminator.add(Dense(units=1024, input_dim=784))\n    discriminator.add(LeakyReLU(0.2))\n    discriminator.add(Dropout(0.3))\n    \n    discriminator.add(Dense(units=512))\n    discriminator.add(LeakyReLU(0.2))\n    discriminator.add(Dropout(0.3))\n    \n    discriminator.add(Dense(units=256))\n    discriminator.add(LeakyReLU(0.2))\n    \n    discriminator.add(Dense(units=1, activation='sigmoid'))\n    \n    discriminator.compile(loss='binary_crossentropy', optimizer=adam_optimizer())\n    return discriminator\n\nd = create_discriminator()\nd.summary()","execution_count":5,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_5 (Dense)              (None, 1024)              803840    \n_________________________________________________________________\nleaky_re_lu_4 (LeakyReLU)    (None, 1024)              0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 1024)              0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 512)               524800    \n_________________________________________________________________\nleaky_re_lu_5 (LeakyReLU)    (None, 512)               0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 256)               131328    \n_________________________________________________________________\nleaky_re_lu_6 (LeakyReLU)    (None, 256)               0         \n_________________________________________________________________\ndense_8 (Dense)              (None, 1)                 257       \n=================================================================\nTotal params: 1,460,225\nTrainable params: 1,460,225\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_gan(generator,discriminator):\n    discriminator.trainable= False\n    gan_input = Input(shape=(100,))\n    x = generator(gan_input)\n    gan_output = discriminator(x)\n    gan = Model(inputs=gan_input, outputs=gan_output)\n    gan.compile(loss='binary_crossentropy', optimizer='adam')\n    return gan\n    \ngan = create_gan(g,d)\ngan.summary()","execution_count":6,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         (None, 100)               0         \n_________________________________________________________________\nsequential_1 (Sequential)    (None, 784)               1486352   \n_________________________________________________________________\nsequential_2 (Sequential)    (None, 1)                 1460225   \n=================================================================\nTotal params: 2,946,577\nTrainable params: 1,486,352\nNon-trainable params: 1,460,225\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to plot the generated images\ndef plot_generated_images(epoch, generator, examples=100, dim=(10,10), figsize=(10,10)):\n    noise = np.random.normal(loc=0, scale=1, size=[examples,100])\n    generated_images = generator.predict(noise)\n    generated_images = generated_images.reshape(100,28,28)\n    plt.figure(figsize=figsize)\n    for i in range(generated_images.shape[0]):\n        plt.subplot(dim[0], dim[1], i+1)\n        plt.imshow(generated_images[i], interpolation='nearest')\n        plt.axis('off')\n    plt.tight_layout()\n    plt.savefig('gan_generated_image %d.png' %epoch)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function for training the data\ndef training(epochs=1,batch_size=128):\n    \n    #Loading the data\n    (X_train,y_train,X_test,y_test) = load_data()\n    batch_count = X_train.shape[0] / batch_size\n    \n    #Creating GAN\n    generator = create_generator()\n    discriminator = create_discriminator()\n    gan = create_gan(generator,discriminator)\n    \n    for e in range(1,epochs+1):\n        print(\"Epoch %d\" %e)\n        for _ in tqdm(range(batch_size)):\n            #Generate random input as an input to initialize the generator\n            noise = np.random.normal(0,1,[batch_size,100])\n            \n            #Generate fake MNIST images from noised input\n            generated_images = generator.predict(noise)\n            \n            #Get a random set of normal images\n            image_batch = X_train[np.random.randint(low=0, high=X_train.shape[0], size = batch_size)]\n            \n            #Construct different batches of real and fake data\n            X = np.concatenate([image_batch, generated_images])\n            \n            #Label for generated and real data\n            y_dis = np.zeros(2*batch_size)\n            y_dis[:batch_size] = 0.9\n            \n            #Pre train discriminator on fake data and real data befor staring the gan\n            discriminator.trainable = True\n            discriminator.train_on_batch(X, y_dis)\n            \n            #Tricking the noise input of the generator as real data\n            noise = np.random.normal(0,1,[batch_size,100])\n            y_gen = np.ones(batch_size)\n            \n            #During the training of the Gan, the weights of discriminator should be fixed\n            discriminator.trainable = False\n            \n            #Training the GAN \n            gan.train_on_batch(noise, y_gen)\n            \n            if e==1 or e%20== 0:\n                plot_generated_images(e,generator) \ntraining(400,128)\n                \n            ","execution_count":null,"outputs":[{"output_type":"stream","text":"\r  0%|          | 0/128 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"Epoch 1\n","name":"stdout"},{"output_type":"stream","text":" 16%|█▌        | 20/128 [01:00<05:06,  2.84s/it]/opt/conda/lib/python3.6/site-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n  max_open_warning, RuntimeWarning)\n100%|██████████| 128/128 [06:13<00:00,  2.59s/it]\n  5%|▌         | 7/128 [00:00<00:01, 69.47it/s]","name":"stderr"},{"output_type":"stream","text":"Epoch 2\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 128/128 [00:01<00:00, 71.91it/s]\n  6%|▋         | 8/128 [00:00<00:01, 78.09it/s]","name":"stderr"},{"output_type":"stream","text":"Epoch 3\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 128/128 [00:01<00:00, 75.69it/s]\n  6%|▋         | 8/128 [00:00<00:01, 74.76it/s]","name":"stderr"},{"output_type":"stream","text":"Epoch 4\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 128/128 [00:01<00:00, 76.89it/s]\n  6%|▋         | 8/128 [00:00<00:01, 77.82it/s]","name":"stderr"},{"output_type":"stream","text":"Epoch 5\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 128/128 [00:01<00:00, 77.64it/s]\n  7%|▋         | 9/128 [00:00<00:01, 79.80it/s]","name":"stderr"},{"output_type":"stream","text":"Epoch 6\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 128/128 [00:01<00:00, 78.44it/s]\n  7%|▋         | 9/128 [00:00<00:01, 80.23it/s]","name":"stderr"},{"output_type":"stream","text":"Epoch 7\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 128/128 [00:01<00:00, 77.86it/s]\n  7%|▋         | 9/128 [00:00<00:01, 80.54it/s]","name":"stderr"},{"output_type":"stream","text":"Epoch 8\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 128/128 [00:01<00:00, 79.32it/s]\n  7%|▋         | 9/128 [00:00<00:01, 81.62it/s]","name":"stderr"},{"output_type":"stream","text":"Epoch 9\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 128/128 [00:01<00:00, 77.73it/s]\n  6%|▋         | 8/128 [00:00<00:01, 79.56it/s]","name":"stderr"},{"output_type":"stream","text":"Epoch 10\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 128/128 [00:01<00:00, 76.89it/s]\n  6%|▋         | 8/128 [00:00<00:01, 76.80it/s]","name":"stderr"},{"output_type":"stream","text":"Epoch 11\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 128/128 [00:01<00:00, 77.60it/s]\n  7%|▋         | 9/128 [00:00<00:01, 81.88it/s]","name":"stderr"},{"output_type":"stream","text":"Epoch 12\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 128/128 [00:01<00:00, 78.09it/s]\n  6%|▋         | 8/128 [00:00<00:01, 78.35it/s]","name":"stderr"},{"output_type":"stream","text":"Epoch 13\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 128/128 [00:01<00:00, 78.46it/s]\n  6%|▋         | 8/128 [00:00<00:01, 79.13it/s]","name":"stderr"},{"output_type":"stream","text":"Epoch 14\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 128/128 [00:01<00:00, 78.05it/s]\n  7%|▋         | 9/128 [00:00<00:01, 80.81it/s]","name":"stderr"},{"output_type":"stream","text":"Epoch 15\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 128/128 [00:01<00:00, 75.64it/s]\n  6%|▋         | 8/128 [00:00<00:01, 77.15it/s]","name":"stderr"},{"output_type":"stream","text":"Epoch 16\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 128/128 [00:01<00:00, 77.49it/s]\n  6%|▋         | 8/128 [00:00<00:01, 79.99it/s]","name":"stderr"},{"output_type":"stream","text":"Epoch 17\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 128/128 [00:01<00:00, 77.45it/s]\n  7%|▋         | 9/128 [00:00<00:01, 83.83it/s]","name":"stderr"},{"output_type":"stream","text":"Epoch 18\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 128/128 [00:01<00:00, 78.95it/s]\n  6%|▋         | 8/128 [00:00<00:01, 70.64it/s]","name":"stderr"},{"output_type":"stream","text":"Epoch 19\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 128/128 [00:01<00:00, 78.44it/s]\n  0%|          | 0/128 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"Epoch 20\n","name":"stdout"},{"output_type":"stream","text":"  5%|▌         | 7/128 [00:17<04:55,  2.44s/it]","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}