{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LoadinG The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 12)\n",
      "(100, 12)\n",
      "<bound method NDFrame.head of          index      Id   ProductId          UserId  \\\n",
      "4447     16085   17572  B0000GH6UQ  A2Q2IWWX5MLU9C   \n",
      "3770    510873  552381  B0007OPW5C  A1BDOVKT96RTYJ   \n",
      "1942    249632  270675  B0000AH9XL  A1DOP5WV948LA0   \n",
      "10206   243963  264554  B000FJJ2IC   AR2S1RYXQ8ST3   \n",
      "7931    307661  333198  B000EQYSOY  A1Y7JCTEBJC7K1   \n",
      "16543   115798  125585  B000P3T4HW  A24BKYGSAPU85J   \n",
      "2937    391781  423661  B0004M0DDS  A1TV3ECUYVVIXP   \n",
      "88820    21312   23333  B000MPFJAO  A11V9GP93SA9MC   \n",
      "27619   132835  144195  B000BBY7YS  A1GVGQGWK8HI03   \n",
      "14788   412946  446608  B000LKZEP0  A1ITT6740FONU4   \n",
      "9665    347922  376346  B000FEFECA  A33CMQWETSAY0X   \n",
      "14652    72682   79112  B000LKYNLG  A2DMK3REP7LEGI   \n",
      "54121   352996  381825  B000FBM494   A2624W1VFNDLI   \n",
      "64275   502065  542819  B000GB0TMW  A1FF072J0HASUR   \n",
      "102505  116216  126023  B000SVFAHE  A3M4CNFB5OF7S8   \n",
      "88430    54167   58800  B000MIFS3Y   ASPZBFGF4AS8H   \n",
      "17465   265071  287333  B000SAPVAQ  A3I4PCBRENJNG2   \n",
      "18325   205892  223107  B000UUWECC  A3VERO2GSGYAML   \n",
      "17368   170728  185223  B000RUIICU  A3ICKK6L4OCA5I   \n",
      "229397   62963   68403  B003EM7J9Q  A1O5EVI1GLFEVN   \n",
      "3700    497434  537780  B0007IQQVC  A1EVQDXJ9XNDMX   \n",
      "13633   214216  232173  B000KENKYE  A38WW7OEEJIVGK   \n",
      "30616    71317   77589  B001LG940E  A1ZBGFVVMDPOPD   \n",
      "145108  294140  318643  B001E5E33U  A2TGQX5MM4J8YD   \n",
      "23142   243826  264400  B0019412BM  A12GK5JUYAV0HM   \n",
      "24358   320051  346462  B001CHH3QY   ABVHIBSM97IUX   \n",
      "138081   89430   97324  B001D69FHY   A36P06J3XULVW   \n",
      "9475    441496  477403  B000FCOOVY  A1A9UHWUQPVPNM   \n",
      "84105   140756  152755  B000LKXRC2  A3N2QDTY3VLUJK   \n",
      "115798  461404  498953  B0010TCFYO  A3LQ3YL5MJA5NU   \n",
      "...        ...     ...         ...             ...   \n",
      "47086   207365  224754  B004CH0YQM  A1URDXDID866D4   \n",
      "300240  521069  563356  B007JFMH8M  A12270M2NQN1QC   \n",
      "290395  482036  521214  B005VI2YBY  A2MNB77YGJ3CN0   \n",
      "34439   519936  562126  B002911NFE   AJ7D9752X5DY7   \n",
      "271984  497246  537582  B004RZZM02  A3AWVUX8F51EWD   \n",
      "10721   338128  365853  B000G0W4LM   AYS9JIZSVP62I   \n",
      "6881    219345  237770  B000EA3M4M   AKVL9VFSI591B   \n",
      "214095  447018  483304  B002UQ74HO  A3GQ0RUUTQLO8T   \n",
      "41980   218386  236742  B003IN848Q  A2OJTGLO8KSWFE   \n",
      "11824   170251  184700  B000H9NFR4  A364UVS5CP9LRY   \n",
      "298230   91629   99673  B0074MXAD6  A15OF2PYL23K06   \n",
      "291815   76818   83571  B005ZBZLT4   AY08FJR13HWNV   \n",
      "41482   216097  234192  B003EMW4ZA  A2JC6UWL0LTLSE   \n",
      "3336     95670  103976  B00065LI0A  A36D2JJR7KK5VL   \n",
      "24911    85843   93470  B001DDD4C4  A1VFPHADX28V3L   \n",
      "236029   57720   62596  B003OP72SQ  A28LHFRE9QHAG3   \n",
      "118350  270338  293056  B0012DFMZ2  A1ARHAUVXWZ0FD   \n",
      "72518   514813  556612  B000HM4LNI  A1MBI7XDXW6EGQ   \n",
      "44512   401648  434285  B003Z6ZGZU  A1UVIRUNV0N3JU   \n",
      "4053     97775  106228  B0000DJT3C  A1HBD3ETVYXIYM   \n",
      "45690    63086   68541  B0044BOZNE  A1UUCCX91WBUIL   \n",
      "250557  202688  219616  B0043WOANY  A2QWA03BG802M1   \n",
      "56879    14794   16139  B008BMDRAE  A1CXJODSRP89JN   \n",
      "242112  368305  398217  B003W59WYA  A1YNR1BTXAZI8M   \n",
      "288244  190315  206363  B005NI7V1U  A2G8MUYP0OQR1P   \n",
      "226371  340029  367872  B003B3OOPA  A3W2I2P178GDT5   \n",
      "292582  504560  545592  B0060ECULW  A3UEDGHRM7MISK   \n",
      "182346  508173  549500  B001RQOY94  A2WOPSEWZHA8L4   \n",
      "252652   13024   14211  B0045XE32E  A10LIGIT9EGCM9   \n",
      "257442   14049   15341  B004AFODLI  A1X89OJFFOVO1Q   \n",
      "\n",
      "                                            ProfileName  HelpfulnessNumerator  \\\n",
      "4447                           T. Butterfield \"lab rat\"                     8   \n",
      "3770                                             Geezer                    11   \n",
      "1942    Little Bear \"booklover, cook, gardener, friend\"                     0   \n",
      "10206                                          N. Yanez                     0   \n",
      "7931                     Sharon K. Haldorsen \"Momzy247\"                     0   \n",
      "16543                                       Sweetie7544                     1   \n",
      "2937                                    D. B. Dahlstrom                     0   \n",
      "88820                                      Darla Taylor                     3   \n",
      "27619                                          K. Avery                     2   \n",
      "14788                                          Samantha                     1   \n",
      "9665                                              Homer                     5   \n",
      "14652                                 LeejaStar \"Leeja\"                     0   \n",
      "54121                                   lenco \"scooter\"                     0   \n",
      "64275                                             Andie                     3   \n",
      "102505               Carolyn J. Allison \"lady of piano\"                     9   \n",
      "88430                                        Marco Polo                     0   \n",
      "17465                                           L. Cain                     1   \n",
      "18325                                            Kahoku                    14   \n",
      "17368                           Chuck Friel \"GatorGrub\"                     3   \n",
      "229397                     L. Herbert \"junkfood junkie\"                     1   \n",
      "3700           Darren S. Eiswirth \"Woodlands Ad Agency\"                     4   \n",
      "13633                                 Patricia A. Kline                     0   \n",
      "30616                                     Mr. Fantastic                     0   \n",
      "145108                                         Hawkshoe                     1   \n",
      "23142                                M. Spencer \"Kitty\"                     0   \n",
      "24358                                             cgirl                     0   \n",
      "138081                                              Val                     0   \n",
      "9475                                            Marlana                     3   \n",
      "84105                                   Supperconductor                     6   \n",
      "115798                              C. Torrillo \"Kring\"                     2   \n",
      "...                                                 ...                   ...   \n",
      "47086                                    Audrey Bennett                     0   \n",
      "300240                               N. Bachu \"New Mom\"                     0   \n",
      "290395                                     L. Mountford                     0   \n",
      "34439                                      Bryan Eckert                     0   \n",
      "271984                                             PJ M                     1   \n",
      "10721                                             yurih                     0   \n",
      "6881                                               Lnk1                     0   \n",
      "214095                                   Laura Halliday                     0   \n",
      "41980                                            ADB127                     0   \n",
      "11824                                h jeffrey schwartz                     0   \n",
      "298230                                            Nancy                     0   \n",
      "291815                                      Aaron Brown                     2   \n",
      "41482                                        FW Shopper                     0   \n",
      "3336                                        julie74brsy                     0   \n",
      "24911                                         Anonymous                     0   \n",
      "236029                                 Randall Souviney                     1   \n",
      "118350                                        Christine                     0   \n",
      "72518                                      Jeff Brandon                     0   \n",
      "44512                                  Kenneth Ferreira                     2   \n",
      "4053                                            Calikym                     0   \n",
      "45690                                           Teysong                     0   \n",
      "250557                                            Dmama                     1   \n",
      "56879                                           Ben Sai                     0   \n",
      "242112                           Andrea Haynes \"Andrea\"                     0   \n",
      "288244                         B. Claggett \"BonnieBlue\"                     0   \n",
      "226371                                               SS                     0   \n",
      "292582                                          ruthann                     0   \n",
      "182346                                         joet1953                     0   \n",
      "252652                                         cocopupu                     0   \n",
      "257442                                             Zach                     0   \n",
      "\n",
      "        HelpfulnessDenominator     Score        Time  \\\n",
      "4447                         9  positive  1104451200   \n",
      "3770                        13  negative  1150416000   \n",
      "1942                         0  positive  1163289600   \n",
      "10206                        0  negative  1173398400   \n",
      "7931                         2  negative  1182816000   \n",
      "16543                        3  negative  1186272000   \n",
      "2937                         2  negative  1186876800   \n",
      "88820                        3  positive  1188432000   \n",
      "27619                        2  positive  1188604800   \n",
      "14788                        3  negative  1192233600   \n",
      "9665                        11  negative  1194048000   \n",
      "14652                        1  negative  1196294400   \n",
      "54121                        0  positive  1199664000   \n",
      "64275                        3  positive  1202428800   \n",
      "102505                      10  positive  1207353600   \n",
      "88430                        0  positive  1207440000   \n",
      "17465                        3  negative  1210291200   \n",
      "18325                       20  negative  1219449600   \n",
      "17368                        4  negative  1222992000   \n",
      "229397                       1  positive  1224720000   \n",
      "3700                         6  negative  1227139200   \n",
      "13633                        3  negative  1227916800   \n",
      "30616                        0  negative  1236816000   \n",
      "145108                       1  positive  1237161600   \n",
      "23142                        0  negative  1242000000   \n",
      "24358                        0  negative  1242864000   \n",
      "138081                       0  positive  1242950400   \n",
      "9475                         4  negative  1243209600   \n",
      "84105                        6  positive  1243555200   \n",
      "115798                       2  positive  1243987200   \n",
      "...                        ...       ...         ...   \n",
      "47086                        2  negative  1341619200   \n",
      "300240                       0  positive  1341964800   \n",
      "290395                       0  positive  1342051200   \n",
      "34439                        0  negative  1343001600   \n",
      "271984                       1  positive  1343260800   \n",
      "10721                        0  negative  1344124800   \n",
      "6881                         0  negative  1344470400   \n",
      "214095                       0  positive  1344729600   \n",
      "41980                        0  negative  1344816000   \n",
      "11824                        0  negative  1345334400   \n",
      "298230                       0  positive  1345420800   \n",
      "291815                       2  positive  1345939200   \n",
      "41482                        0  negative  1346889600   \n",
      "3336                         0  negative  1346889600   \n",
      "24911                        0  negative  1347321600   \n",
      "236029                       1  positive  1347494400   \n",
      "118350                       0  positive  1347580800   \n",
      "72518                        0  positive  1347926400   \n",
      "44512                        2  negative  1348185600   \n",
      "4053                         0  positive  1348358400   \n",
      "45690                        0  negative  1348358400   \n",
      "250557                       1  positive  1348531200   \n",
      "56879                        2  negative  1348617600   \n",
      "242112                       0  positive  1348963200   \n",
      "288244                       0  positive  1349049600   \n",
      "226371                       0  positive  1349827200   \n",
      "292582                       0  positive  1350345600   \n",
      "182346                       0  positive  1350345600   \n",
      "252652                       0  positive  1350518400   \n",
      "257442                       0  positive  1350604800   \n",
      "\n",
      "                                                  Summary  \\\n",
      "4447                                             The Best   \n",
      "3770                                        Weak flavor!!   \n",
      "1942                    Your Cat Will Lick the Bowl Clean   \n",
      "10206                                   A waste of money!   \n",
      "7931             Not as good as some of their other chips   \n",
      "16543                                               Yuck!   \n",
      "2937                                  worst popcorn ever?   \n",
      "88820                                 sweet and sour pops   \n",
      "27619                         Very nice and fresh flowers   \n",
      "14788                                Not like real cheese   \n",
      "9665                                            Bad Taste   \n",
      "14652                                        Not So Tasty   \n",
      "54121                              Familia Low Fat Museli   \n",
      "64275                                             Superb!   \n",
      "102505                          Great Rice Breakfast food   \n",
      "88430                  Great \"no brainer\" lunch or dinner   \n",
      "17465      Davidson's Tea Spiced Pear, 100-Count Tea Bags   \n",
      "18325                                             Hate It   \n",
      "17368                       Didn't Repel any critters..!!   \n",
      "229397                                    warm and crispy   \n",
      "3700                                  Weak, Weak, Weak...   \n",
      "13633         Bob's Red Mill Gluten-Free Cinnamon Bun Mix   \n",
      "30616                                               weird   \n",
      "145108                                A great cup of tea.   \n",
      "23142                                  Utterly Flavorless   \n",
      "24358            Cannot really taste the hazelnut flavor.   \n",
      "138081                            Simply one of the best!   \n",
      "9475                                      Useless product   \n",
      "84105                    Excellent Low-Sodium Alternative   \n",
      "115798                      Best tasting high protein bar   \n",
      "...                                                   ...   \n",
      "47086                          5 seconds of flavor...  :(   \n",
      "300240                                   Soft and Delish!   \n",
      "290395                             Nice peachy flavor ...   \n",
      "34439                                           Terrible.   \n",
      "271984  Not as good as the meat version, but quite goo...   \n",
      "10721         Maybe good cracker, but package was damaged   \n",
      "6881    Empty packets and grains clumped and stuck tog...   \n",
      "214095                                  Great for sun tea   \n",
      "41980                                      Good but weak.   \n",
      "11824                                   Maltitol is toxic   \n",
      "298230                                         Fantastic!   \n",
      "291815                                       Great Coffee   \n",
      "41482                                          Misleading   \n",
      "3336                                                   ok   \n",
      "24911                  Not balsamic, it's not even close!   \n",
      "236029           Great flavor, low acid, reasonable price   \n",
      "118350            the best chocolate extract I ever used!   \n",
      "72518                       Different wrapper, same candy   \n",
      "44512                                   Very disappointed   \n",
      "4053         Tasty, adorable and none broken upon arrival   \n",
      "45690                                   My son hates this   \n",
      "250557                                      free at last!   \n",
      "56879                                         Never Again   \n",
      "242112                               delicious, easy, fun   \n",
      "288244                                           Love 'em   \n",
      "226371                                                Raw   \n",
      "292582                                      k cups coffee   \n",
      "182346                                Natural hog casings   \n",
      "252652                      Yes these are good per my dog   \n",
      "257442                                            Perfect   \n",
      "\n",
      "                                                     Text  \\\n",
      "4447    It is the best hot chocolate I have ever taste...   \n",
      "3770    This used to be the bomb back in the day...hot...   \n",
      "1942    I have three adopted felines friends and will ...   \n",
      "10206   My cat doesn't like them at all. I have even t...   \n",
      "7931    I didn't care for these chips as much as I did...   \n",
      "16543   I'm always on the look out for low calorie/low...   \n",
      "2937    This popcorn left an unpleasant-tasting layer ...   \n",
      "88820   Just as I remember from my childhood, though a...   \n",
      "27619   I received this very nice bouquet of flowers. ...   \n",
      "14788   I bought this because I'm vegan and do not eat...   \n",
      "9665    I couldn't detect any raspberry flavor at all,...   \n",
      "14652   Out of the several brands of canned refried be...   \n",
      "54121   Have been eating this breakfast cereal for sev...   \n",
      "64275   This matcha green tea is perfect for the price...   \n",
      "102505  <a href=\"http://www.amazon.com/gp/product/B000...   \n",
      "88430   Way too good to be a microwave meal.  I was ra...   \n",
      "17465   ok, but not what I expected.<br />Lots of loos...   \n",
      "18325   This product taste nothing like fresh coconut ...   \n",
      "17368   Product didn't work as advertised. Mixed solut...   \n",
      "229397  If you like pork rinds you're going to love th...   \n",
      "3700    I'm a strong coffee drinker and thought to mys...   \n",
      "13633   Hard to work with, but no less than what I exp...   \n",
      "30616   I thought this was very acidic and tart.  I do...   \n",
      "145108  I love a cup of bold black tea to start off my...   \n",
      "23142   Unfortunately, these particular snacks are jus...   \n",
      "24358   I am disappointed in that I cannot really tast...   \n",
      "138081  This is simply one of the best vinegar for the...   \n",
      "9475    I bought the scoop and bags mostly for the bag...   \n",
      "84105   I had given up using bullion cubes due to thei...   \n",
      "115798  I believe I've tried just about every bar in e...   \n",
      "...                                                   ...   \n",
      "47086   Not worth the money or time- bummer. I would n...   \n",
      "300240  I received this cookie in my mom voxbox. This ...   \n",
      "290395  I love this stuff. I like anything peach -- ic...   \n",
      "34439   I'll admit I bought these for the jar. They we...   \n",
      "271984  My wife is vegetarian and I was eating some tr...   \n",
      "10721   This may be a good cracker, but I had to retur...   \n",
      "6881    About 40% packets had no sugar in them and a b...   \n",
      "214095  I brew this in a sun tea jar and it is fruity ...   \n",
      "41980   If you would like weak coffee use one packet i...   \n",
      "11824   These items contain a great deal of maltitol, ...   \n",
      "298230  This is absolutely delicious! My husband has h...   \n",
      "291815  I generally like the stronger coffees.  A lot ...   \n",
      "41482   Beware -- This is a bulk food item. These are ...   \n",
      "3336    This was very nice in the pic which is why I p...   \n",
      "24911   Due to the low price and good reviews, we thou...   \n",
      "236029  We have tried many ground espresso products an...   \n",
      "118350  I have never used a chocolate flavor that has ...   \n",
      "72518   I don't care for the new wrapper, but the cand...   \n",
      "44512   Very weak coffee, just a hint of pumpkin spice...   \n",
      "4053    They are tasty and cute!  The kids (10 year ol...   \n",
      "45690   I am not sure what the problem is with this on...   \n",
      "250557  This product has helped me break free from the...   \n",
      "56879   This is the most pitiful excuse for a bonsai t...   \n",
      "242112  These caramel wraps are very good and fun for ...   \n",
      "288244  My husband and I tried the crisps at a party, ...   \n",
      "226371  Using this product as a natural moisturizer fo...   \n",
      "292582  purchased for my college granddaughter....she ...   \n",
      "182346  Product was very easy to use even for a first ...   \n",
      "252652  okay how to review dog treats?  Of course my d...   \n",
      "257442  THere's nothing else I can say. This mix is aw...   \n",
      "\n",
      "                                              CleanedText  \n",
      "4447      b'best hot chocol ever tast cinnamon make good'  \n",
      "3770       b'use bomb back day hot flavor weak splinteri'  \n",
      "1942    b'three adopt felin friend first admit spoil r...  \n",
      "10206   b'cat doesnt like even tri refus eat place wit...  \n",
      "7931    b'didnt care chip much sun dri tomato one diff...  \n",
      "16543   b'alway look low calori low point healthi alte...  \n",
      "2937    b'popcorn left layer waxi substanc built teeth...  \n",
      "88820       b'rememb childhood though quarter box broken'  \n",
      "27619   b'receiv nice bouquet flower arriv fresh readi...  \n",
      "14788   b'bought vegan eat dairi hope would good chees...  \n",
      "9665    b'couldnt detect raspberri flavor like prune w...  \n",
      "14652   b'sever brand can refri bean tri least favorit...  \n",
      "54121   b'eat breakfast cereal sever year think excel ...  \n",
      "64275   b'matcha green tea perfect price quantiti bake...  \n",
      "102505  b'cream rice cereal unit pack dialysi patient ...  \n",
      "88430   b'way good microwav meal rais italian food nev...  \n",
      "17465   b'expect lot loos tea bag box would made good ...  \n",
      "18325   b'product tast noth like fresh coconut juic fu...  \n",
      "17368   b'product didnt work advertis mix solut strict...  \n",
      "229397  b'like pork rind your go love come microwav wa...  \n",
      "3700    b'strong coffe drinker thought french roast bo...  \n",
      "13633   b'hard work less expect heard like abl make st...  \n",
      "30616   b'thought acid tart dont think carbon favor ei...  \n",
      "145108  b'love cup bold black tea start day twine engl...  \n",
      "23142   b'unfortun particular snack flavorless order s...  \n",
      "24358   b'disappoint cannot realli tast hazelnut flavo...  \n",
      "138081      b'simpli one best vinegar price truli addict'  \n",
      "9475    b'bought scoop bag most bag earth friend bag s...  \n",
      "84105   b'given use bullion cube due toxic level sodiu...  \n",
      "115798  b'believ ive tri everi bar exist found bar loc...  \n",
      "...                                                   ...  \n",
      "47086   b'worth money bummer would recommend type gum ...  \n",
      "300240  b'receiv cooki mom voxbox cooki soft delici ki...  \n",
      "290395  b'love stuff like anyth peach ice cream yogurt...  \n",
      "34439   b'ill admit bought jar sale local supermarket ...  \n",
      "271984  b'wife vegetarian eat tradit can chili said wi...  \n",
      "10721   b'may good cracker return sinc insid wrap rip ...  \n",
      "6881    b'packet sugar bunch other open leak sugar gra...  \n",
      "214095  b'brew sun tea jar fruiti rich elderberri give...  \n",
      "41980   b'would like weak coffe use one packet would l...  \n",
      "11824   b'item contain great deal maltitol inflict ter...  \n",
      "298230  b'absolut delici husband vanilla ice cream eve...  \n",
      "291815  b'general like stronger coffe lot weak flavorl...  \n",
      "41482   b'bewar bulk food item individu wrap shown pic...  \n",
      "3336    b'nice pic pick sent two actual well gift much...  \n",
      "24911   b'due low price good review thought wed give b...  \n",
      "236029  b'tri mani ground espresso product like best s...  \n",
      "118350  b'never use chocol flavor natur bake product w...  \n",
      "72518   b'dont care new wrapper candi tast exact bulk ...  \n",
      "44512   b'weak coffe hint pumpkin dunkin donut pumpkin...  \n",
      "4053    b'tasti cute kid year old tri take mani possib...  \n",
      "45690   b'sure problem one mayb ginger love oth sprout...  \n",
      "250557  b'product help break free peanut butter habit ...  \n",
      "56879   b'piti excus bonsai ever seen life consist twi...  \n",
      "242112  b'caramel wrap good fun whole famili togeth ho...  \n",
      "288244  b'husband tri crisp parti love first bite smal...  \n",
      "226371  b'use product natur moistur hair one best deci...  \n",
      "292582  b'purchas colleg granddaught love starbuck cof...  \n",
      "182346  b'product easi use even first timer great qual...  \n",
      "252652  b'okay review dog treat cours dog went crazi d...  \n",
      "257442  b'there noth els say mix awesom good without a...  \n",
      "\n",
      "[200 rows x 12 columns]>\n",
      "(200, 12)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "from sklearn import metrics\n",
    "\n",
    "#using the preprocess Sqlite Table to read the  data\n",
    "con = sqlite3.connect('./amazon-fine-food-reviews/finalsqlite')\n",
    "\n",
    "#Filtering only positive review\n",
    "positive_review = pd.read_sql_query(\"\"\"\n",
    "SELECT * FROM Reviews Where Score == 'positive'\n",
    "\"\"\", con)\n",
    "\n",
    "#Extracting 2000 Positive review Randomly\n",
    "positive_smaller_review_set = positive_review.sample(100)\n",
    "print(positive_smaller_review_set.shape)\n",
    "\n",
    "#Filtering only negative review\n",
    "negative_review = pd.read_sql_query(\"\"\"\n",
    "SELECT * FROM Reviews Where Score == 'negative'\n",
    "\"\"\", con)\n",
    "\n",
    "#Extracing 2000 random negative review\n",
    "negative_smaller_review_set = negative_review.sample(100)\n",
    "print(negative_smaller_review_set.shape)\n",
    "\n",
    "#Concanating Negative and Positive review into one dataframe\n",
    "frames = [positive_smaller_review_set, negative_smaller_review_set]\n",
    "final = pd.concat(frames)\n",
    "\n",
    "#Sort the dataframe data into the increasing Time order\n",
    "final = final.sort_values(by=['Time'])\n",
    "print(final.head)\n",
    "print(final.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to  find accuracy and optimal alpha  on given vector\n",
    "\n",
    "def Naive_Bayes_accuracy(X_train, y_train):\n",
    "    \n",
    "    #List of alpha values to check for optimal alpha value\n",
    "    alphas =  np.array([0.00001, 0.00003, 0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10])\n",
    "\n",
    "    cv_scores = []\n",
    "     \n",
    "    # perform 10 fold cross validation\n",
    "    for k in alphas:\n",
    "        clf = MultinomialNB(alpha = k )\n",
    "        scores = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')\n",
    "        cv_scores.append(scores.mean())\n",
    "\n",
    "    # changing to misclassification error\n",
    "    MSE = [1 - x for x in cv_scores]\n",
    "    print(MSE)\n",
    "    # determining best alpha\n",
    "    optimal_alpha = alphas[MSE.index(min(MSE))]\n",
    "    print('\\nThe optimal value of alpha is %d.' % optimal_alpha)\n",
    "    \n",
    "    # Feature Importance\n",
    "    \n",
    "    return optimal_alpha\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag OF word(BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of vector of Preprocess review(text) after BOW(unigram) (200, 1833)\n"
     ]
    }
   ],
   "source": [
    "#BOW of preprocess text\n",
    "count_vect = CountVectorizer()\n",
    "final_counts_preprocess_text = count_vect.fit_transform(final['CleanedText'].values)\n",
    "\n",
    "print(\"Shape of vector of Preprocess review(text) after BOW(unigram)\",final_counts_preprocess_text.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test data Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1833)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "# create design matrix X and target vector y\n",
    "\n",
    "X = np.array(final_counts_preprocess_text.toarray()[:,:])  \n",
    "y = np.array(final['Score'].values)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "# Calculating the train and test size and dividing the data into train and test chunks into the 70 - 30 ratio\n",
    "train_size = math.floor(X.shape[0] * 0.7)\n",
    "\n",
    "#split the data set into train and test\n",
    "X_train = X[: train_size ,:]\n",
    "X_test  = X[train_size : , :]  \n",
    "y_train = y[: train_size ]\n",
    "y_test =  y[train_size : ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Naive Bayes on BOW vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2695604395604395, 0.2695604395604395, 0.2695604395604395, 0.2695604395604395, 0.2624175824175824, 0.2624175824175824, 0.2624175824175824, 0.2552747252747253, 0.24194139194139197, 0.27208791208791205, 0.28589743589743577, 0.29468864468864464, 0.3017582417582416]\n",
      "\n",
      "The optimal value of alpha is 0.\n",
      "['negative' 'positive']\n",
      "log probability for words with higher probability for negative reviews is   tast\n",
      "log probability for words with higher probability for positive reviews is  like\n",
      "[0 1 1 0 0 0 1 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0\n",
      " 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1]\n",
      "[0 0 1 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 1 0 1 1 0\n",
      " 0 1 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of unknown and binary targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-614eebf73326>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'multilabel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[0mdiffering_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 81\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of unknown and binary targets"
     ]
    }
   ],
   "source": [
    "# Using Naive_Bayes_accuracy Function finding optimal alpha \n",
    "from sklearn.metrics import accuracy_score\n",
    "optimal_alpha =  Naive_Bayes_accuracy(X_train, y_train)\n",
    "\n",
    "#calculate the accuracy on Test data\n",
    "\n",
    "clf = MultinomialNB(alpha = optimal_alpha)\n",
    "\n",
    "# fitting the model\n",
    "nb = clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicted Probability of different classes in Ordering\n",
    "print(clf.classes_)\n",
    "\n",
    "# Calculate the log probability of  the test data\n",
    "log_proba = clf.predict_log_proba(X_test)\n",
    "\n",
    "#Calculate the Class \n",
    "y_pred = np.argmax(log_proba, axis=1) \n",
    "\n",
    "#Converting y_test array from positive/Neagtive to  1/0 array\n",
    "y_test[np.where(y_test == 'positive')] = 1\n",
    "y_test[np.where(y_test == 'negative')] = 0\n",
    "\n",
    "# Features After BOW\n",
    "count_vect = count_vect.get_feature_names()\n",
    "\n",
    "#log probability. for words with higher probability for both positive and negative reviews\n",
    "word_probability = np.argmax(nb.feature_log_prob_, axis=1) \n",
    "print(\"log probability for words with higher probability for negative reviews is  \", count_vect[word_probability[0]])\n",
    "print(\"log probability for words with higher probability for positive reviews is \", count_vect[word_probability[1]])\n",
    "\n",
    "print(y_test)\n",
    "print(y_pred)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "# Accuracy , precision, recall, f1 score, confusion matrix for Optimal value of alpha\n",
    "# print(\"\\nThe accuracy of the Naive Bayes classifier on Bag Of Words for optimal_alpha is \",metrics.accuracy_score(y_test, y_pred))\n",
    "# print(\"\\nThe confusion_matrix of the Naive Bayes classifier on Bag Of Words for optimal_alpha is \",metrics.confusion_matrix(y_test, y_pred))\n",
    "# print(\"\\nThe recall_score of the Naive Bayes classifier on Bag Of Words for optimal_alpha is \",metrics.recall_score(y_test, y_pred))\n",
    "# print(\"\\nThe precision_score of the Naive Bayes classifier on Bag Of Words for optimal_alpha is \",metrics.precision_score(y_test, y_pred))\n",
    "# print(\"\\nThe f1_score of the Naive Bayes classifier on Bag Of Words for optimal_alpha is \",metrics.f1_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of vector of Preprocess review(text) after Tf-Idf (100, 7274)\n"
     ]
    }
   ],
   "source": [
    "#tf-idf of preprocess text\n",
    "tf_idf_vect  = TfidfVectorizer(ngram_range=(1,2))\n",
    "final_tf_idf_preprocess_text =  tf_idf_vect.fit_transform(final['CleanedText'].values)\n",
    "\n",
    "#Shape of  vectors\n",
    "print(\"Shape of vector of Preprocess review(text) after Tf-Idf\",final_tf_idf_preprocess_text.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create design matrix X and target vector y\n",
    "\n",
    "X = np.array(final_tf_idf_preprocess_text.toarray()[:,:])  \n",
    "y = np.array(final['Score'].values)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "# Calculating the train and test size and dividing the data into train and test chunks into the 70 - 30 ratio\n",
    "train_size = math.floor(X.shape[0] * 0.7)\n",
    "\n",
    "#split the data set into train and test\n",
    "X_train = X[: train_size ,:]\n",
    "X_test  = X[train_size : , :]  \n",
    "y_train = y[: train_size ]\n",
    "y_test =  y[train_size : ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes on TF-IDF vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brute\n",
      "[0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5267857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.4708333333333333, 0.5, 0.5125, 0.4976190476190475, 0.4976190476190476]\n",
      "\n",
      "The optimal number of neighbors is 21.\n",
      "\n",
      "The accuracy of the knn classifier on Tf-Idf using Brute Force approach for k = 21 is 53.333333%\n",
      "kd_tree\n",
      "[0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5267857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.4708333333333333, 0.5, 0.5125, 0.4976190476190475, 0.4976190476190476]\n",
      "\n",
      "The optimal number of neighbors is 21.\n",
      "\n",
      "The accuracy of the knn classifier on Tf-Idf using K_D tree approach for k = 21 is 53.333333%\n"
     ]
    }
   ],
   "source": [
    "# Using Naive_Bayes_accuracy Function finding optimal alpha \n",
    "from sklearn.metrics import accuracy_score\n",
    "optimal_alpha =  Naive_Bayes_accuracy(X_train, y_train)\n",
    "\n",
    "#calculate the accuracy on Test data\n",
    "\n",
    "clf = MultinomialNB(alpha = optimal_alpha)\n",
    "\n",
    "# fitting the model\n",
    "nb = clf.fit(X_train, y_train)\n",
    "\n",
    "# Predicted Probability of different classes in Ordering\n",
    "print(clf.classes_)\n",
    "\n",
    "# Calculate the log probability of  the test data\n",
    "log_proba = clf.predict_log_proba(X_test)\n",
    "\n",
    "#Calculate the Class \n",
    "y_pred = np.argmax(log_proba, axis=1) \n",
    "\n",
    "#Converting y_test array from positive/Neagtive to  1/0 array\n",
    "y_test[np.where(y_test == 'positive')] = 1\n",
    "y_test[np.where(y_test == 'negative')] = 0\n",
    "\n",
    "# Features After Naive Bayes\n",
    "tf_idf_vect_features = tf_idf_vect.get_feature_names()\n",
    "\n",
    "#log probability. for words with higher probability for both positive and negative reviews\n",
    "word_probability = np.argmax(nb.feature_log_prob_, axis=1) \n",
    "print(\"log probability for words with higher probability for negative reviews is  \", tf_idf_vect_features[word_probability[0]])\n",
    "print(\"log probability for words with higher probability for positive reviews is \", tf_idf_vect_features[word_probability[1]])\n",
    "\n",
    "print(y_test)\n",
    "print(y_pred)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "# Accuracy , precision, recall, f1 score, confusion matrix for Optimal value of alpha\n",
    "# print(\"\\nThe accuracy of the Naive Bayes classifier on Bag Of Words for optimal_alpha is \",metrics.accuracy_score(y_test, y_pred))\n",
    "# print(\"\\nThe confusion_matrix of the Naive Bayes classifier on Bag Of Words for optimal_alpha is \",metrics.confusion_matrix(y_test, y_pred))\n",
    "# print(\"\\nThe recall_score of the Naive Bayes classifier on Bag Of Words for optimal_alpha is \",metrics.recall_score(y_test, y_pred))\n",
    "# print(\"\\nThe precision_score of the Naive Bayes classifier on Bag Of Words for optimal_alpha is \",metrics.precision_score(y_test, y_pred))\n",
    "# print(\"\\nThe f1_score of the Naive Bayes classifier on Bag Of Words for optimal_alpha is \",metrics.f1_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
