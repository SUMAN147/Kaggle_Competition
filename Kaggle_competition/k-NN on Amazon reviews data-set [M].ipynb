{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LoadinG The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DANAV\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 12)\n",
      "(50, 12)\n",
      "<bound method NDFrame.head of          index      Id   ProductId          UserId  \\\n",
      "21125   510894  552406  B0007OPW5C  A393SW83L4WH3G   \n",
      "16501    63064   68516  B0005YWSYM  A1IL6W1NK05UW9   \n",
      "7834    125523  136190  B000EPOC1Y  A1WX42M589VAMQ   \n",
      "73503   395103  427242  B000HZLPRU  A22I82BMS8S5UJ   \n",
      "73894   135839  147428  B000I4PXBO   AUUMSYICW2I0K   \n",
      "10235   120889  131052  B0001LW132  A158HXAKJ150LZ   \n",
      "12061   296733  321431  B000HDKZJQ  A2KR0I0OG4OI0D   \n",
      "50166    72612   79032  B000F1X740   ABAPF3XY1W6JX   \n",
      "102471   80533   87562  B000SV7B76   AXSH8IKKHRWRW   \n",
      "1511    376805  407450  B000084F04  A3J0OXB9KIC5SS   \n",
      "5689    475816  514550  B000CQ6KTM  A3B7QUFEZ9IMBK   \n",
      "19381   499672  540244  B0006SDRRE   AF3CAM6HWRGKY   \n",
      "30724    15321   16727  B001LGGH40  A2CIQEY05SPHTZ   \n",
      "30659     6196    6732  B001LG945O  A37USOKUTI724H   \n",
      "17672   493570  533627  B000SATIWI  A2MLCH9REYK6Q7   \n",
      "160956    8718    9550  B001EQ5IPQ   A11V7W73DYY0A   \n",
      "70582   460225  497702  B000HDK0D2   AQVEZ02KHPCBH   \n",
      "120275  236327  256359  B0013L2FJY   AF62OFMKFKYYU   \n",
      "4066    377641  408357  B00099XNZ6  A1H31XV4OODPAK   \n",
      "94767   294724  319269  B000OU5EFW  A2TV95C03KKVNM   \n",
      "36093    95870  104192  B002FYAONM  A3O4W8D596UXM8   \n",
      "140402  196657  213166  B001E5DRBO  A3FI4OAOQYQWHH   \n",
      "215286  219610  238059  B002WDCAJ2   AB20GD8IZICAM   \n",
      "2070    482041  521219  B0002564JO   A5TMN9MLC1UPN   \n",
      "200926   78607   85486  B002DHTWNO   AV2RRL4NWRKKC   \n",
      "49848   306275  331680  B000F0DW1E  A34ZJXMBAGWZ5U   \n",
      "39492   305114  330445  B0030Z8SLK  A1WJW9XHR2V1WV   \n",
      "32608   248563  269510  B001SBAS34  A398WOSZC1DREY   \n",
      "226965  166120  180129  B003BI2HK4   A3UR8MBDZSBB2   \n",
      "18417   335253  362750  B000UXA0LG  A1RRIREIPB1YLG   \n",
      "...        ...     ...         ...             ...   \n",
      "46939   389888  421568  B000EVG8FQ  A2FNTZVE5KCQL1   \n",
      "2669    184403  200028  B0000D94M3  A3525107IVEY7N   \n",
      "224184  209862  227450  B003832GRQ   ATPZ1PALA2S2Z   \n",
      "35419    73737   80237  B002C58RXI    ACQET5AAIJE5   \n",
      "247953  128152  139061  B0041CIR62  A17D8RDGKBZ5TX   \n",
      "295640  383841  415074  B006H7F0VE  A37U44QN3A3DJN   \n",
      "274455  273136  296039  B004UKEO4E  A15BUTT0ZWU2M8   \n",
      "32438   109518  118855  B001SAUUD8  A1P086IKE1LIAM   \n",
      "53395    74674   81235  B005KMUNT6   AYLT9SZPQE6AY   \n",
      "3485    250621  271738  B0000DGFCO  A3I2TV2LJ4E0SR   \n",
      "2212    206902  224244  B0002ARQYQ   AG3K18NWEHHBM   \n",
      "55443   363981  393647  B000FDN6Q2  A1EWL5OYM5ZCXS   \n",
      "52647    23328   25521  B005EKI67U  A1BK00W6TCYZR9   \n",
      "50223   319538  345917  B004TJF3BE  A35QRUSGHDD8ZR   \n",
      "226856  340253  368112  B003B3OOPA  A3T1LD0C65QCWK   \n",
      "23460   469240  507397  B0009F3QKM   AZQ4MBGFAEL26   \n",
      "55707   131248  142510  B007EDRCVG  A30DDJ2ANV0UN1   \n",
      "62217   478081  516986  B000G1PAC6  A1Q4M7TN3SYSX3   \n",
      "56306   259783  281623  B007RTR9DS  A1437CAM9CNC1E   \n",
      "10510    59264   64382  B000FPJ4M0   AW41Q5K4R499D   \n",
      "21862   364594  394289  B0015DA1HI   AT8MONN9BAYJS   \n",
      "299011  375773  406327  B007B9UETK  A1N5WPY2QAMI83   \n",
      "170709   78086   84923  B001J3ZUQ6   AP71MD612CA7T   \n",
      "5957     45733   49760  B000CRHQN0   A4FJHPDSQQ9XK   \n",
      "40195   442395  478349  B00362FJD2  A2LWAV5B1QOBQB   \n",
      "301106  265713  288036  B007JT7AEY  A31P4BOR51YD0C   \n",
      "47858     8153    8921  B004FZX2YC   AFV1DRHE0S4NN   \n",
      "56479   359348  388682  B007TGO1TY   ARYSDAZNRXN6G   \n",
      "48640    61853   67194  B004JRXZKM  A2F48EVKXC1P9H   \n",
      "17431   504642  545678  B000SANS1U  A1LS1LAD7NPUBE   \n",
      "\n",
      "                                     ProfileName  HelpfulnessNumerator  \\\n",
      "21125                              mcHaiku \"nmi\"                     9   \n",
      "16501                          C. Baker \"cbaker\"                     4   \n",
      "7834                                         Mir                     2   \n",
      "73503                                  Iqueequeg                     2   \n",
      "73894                 Gail B. Combs \"MotownGirl\"                     3   \n",
      "10235                                   lbphilly                     1   \n",
      "12061                                   Nanna 44                     1   \n",
      "50166                              TucsonShopper                     5   \n",
      "102471                    L. Williams \"lynwilly\"                     1   \n",
      "1511                            WILLIAM H FULLER                    66   \n",
      "5689                             Melissa from TX                     8   \n",
      "19381                             Joanna Carroll                     0   \n",
      "30724   Michael Countryman \"mike the camera man\"                     0   \n",
      "30659                    Donna Capshaw \"Grandma\"                     0   \n",
      "17672                                       Zoli                     3   \n",
      "160956                                  SoPasGal                     5   \n",
      "70582                           Jennifer Pokorny                     2   \n",
      "120275                  H. Joffe \"I love movies\"                     2   \n",
      "4066                                   Cassandra                     1   \n",
      "94767                  Goose \"Freighter Captian\"                     5   \n",
      "36093                 W. Mitchell \"minnie mouse\"                     2   \n",
      "140402               P. Woodland \"Broken Teepee\"                     1   \n",
      "215286                                Debra Hunt                     0   \n",
      "2070                                    C. A. M.                     5   \n",
      "200926                       Andrew J. Bergstrom                     1   \n",
      "49848                                   R. Smith                     1   \n",
      "39492                Shakeitlikeapolaroidpicture                     0   \n",
      "32608          William J. Baldwin \"Bill Baldwin\"                     1   \n",
      "226965                                       Kat                     0   \n",
      "18417                            Maryann Watkins                     6   \n",
      "...                                          ...                   ...   \n",
      "46939                             Glutenfreegirl                     0   \n",
      "2669                                        gene                     0   \n",
      "224184                                      Leon                     9   \n",
      "35419                                 FrancoTech                     0   \n",
      "247953                            John W. Loftus                     1   \n",
      "295640                                    Aungie                     0   \n",
      "274455                                     light                     2   \n",
      "32438                            cocktail mistro                     0   \n",
      "53395                                     Nanlou                     0   \n",
      "3485                                   neosapien                     0   \n",
      "2212                                  AkimboPBRs                     0   \n",
      "55443                          Jennifer Fuselier                     0   \n",
      "52647                             Sharon M. Cole                     1   \n",
      "50223                                   Paradise                     0   \n",
      "226856                          Gordon M. Wagner                     6   \n",
      "23460                                   Meg Katz                     0   \n",
      "55707              Grumpysgal in Va \"grumpysgal\"                     1   \n",
      "62217                          Jeffrey L. Watson                     0   \n",
      "56306                  Kate August \"Bookdweller\"                     0   \n",
      "10510                                   mom of 2                     0   \n",
      "21862                                  Stephanie                     0   \n",
      "299011                                      Elle                     0   \n",
      "170709                               J. Gonzales                     0   \n",
      "5957                            asdlkjaspodifuwe                     0   \n",
      "40195                         Speaker to Animals                     0   \n",
      "301106                                     Gizmo                     0   \n",
      "47858                                   D. Mutti                     0   \n",
      "56479                                 A. Crafton                     1   \n",
      "48640                                   K Klimas                     0   \n",
      "17431                                      Linda                     0   \n",
      "\n",
      "        HelpfulnessDenominator     Score        Time  \\\n",
      "21125                       21  positive  1139875200   \n",
      "16501                        4  positive  1147219200   \n",
      "7834                         4  negative  1165363200   \n",
      "73503                        2  positive  1189728000   \n",
      "73894                        3  positive  1191456000   \n",
      "10235                        2  positive  1202083200   \n",
      "12061                        2  negative  1202601600   \n",
      "50166                        5  positive  1207612800   \n",
      "102471                       1  positive  1209600000   \n",
      "1511                        66  positive  1224460800   \n",
      "5689                        15  negative  1232150400   \n",
      "19381                        0  positive  1234742400   \n",
      "30724                        0  negative  1237248000   \n",
      "30659                        0  negative  1237334400   \n",
      "17672                        3  negative  1237420800   \n",
      "160956                       5  positive  1245024000   \n",
      "70582                        3  positive  1249948800   \n",
      "120275                       2  positive  1253923200   \n",
      "4066                         3  negative  1254355200   \n",
      "94767                        5  positive  1256515200   \n",
      "36093                        4  negative  1260576000   \n",
      "140402                       1  positive  1261785600   \n",
      "215286                       1  positive  1262908800   \n",
      "2070                         8  negative  1262995200   \n",
      "200926                       1  positive  1263254400   \n",
      "49848                        1  positive  1266192000   \n",
      "39492                        0  negative  1268784000   \n",
      "32608                        2  negative  1271894400   \n",
      "226965                       0  positive  1276128000   \n",
      "18417                        6  negative  1277769600   \n",
      "...                        ...       ...         ...   \n",
      "46939                        0  positive  1326153600   \n",
      "2669                         0  positive  1327708800   \n",
      "224184                       9  positive  1329436800   \n",
      "35419                        0  negative  1329436800   \n",
      "247953                       2  positive  1329436800   \n",
      "295640                       0  positive  1330732800   \n",
      "274455                       2  positive  1332547200   \n",
      "32438                        0  negative  1334620800   \n",
      "53395                        1  negative  1335571200   \n",
      "3485                         0  positive  1335657600   \n",
      "2212                         0  negative  1336089600   \n",
      "55443                        0  positive  1338940800   \n",
      "52647                        3  negative  1339113600   \n",
      "50223                        0  negative  1339804800   \n",
      "226856                       6  positive  1342310400   \n",
      "23460                        0  positive  1342569600   \n",
      "55707                        2  negative  1342915200   \n",
      "62217                        0  positive  1343347200   \n",
      "56306                        0  negative  1344556800   \n",
      "10510                        2  negative  1344643200   \n",
      "21862                        0  negative  1346457600   \n",
      "299011                       0  positive  1346544000   \n",
      "170709                       0  positive  1346976000   \n",
      "5957                         0  negative  1347494400   \n",
      "40195                        1  negative  1348617600   \n",
      "301106                       0  positive  1348963200   \n",
      "47858                        0  negative  1349136000   \n",
      "56479                        2  negative  1349222400   \n",
      "48640                        0  negative  1349913600   \n",
      "17431                        0  negative  1350604800   \n",
      "\n",
      "                                                  Summary  \\\n",
      "21125         A  HOT  ITEM  FOR  VALENTINE's  DAY  2006 !   \n",
      "16501                                     Super Hot Sauce   \n",
      "7834    Not Very Chocolately, A disappointing texture ...   \n",
      "73503                                      Best K-cup yet   \n",
      "73894                                              Yummy!   \n",
      "10235      Superlative comfort tea for the dessert addict   \n",
      "12061                                   Box is misleading   \n",
      "50166                            Loved by adults and kids   \n",
      "102471                                 Great for fish too   \n",
      "1511                           One Observation Among Many   \n",
      "5689                            These are completely vile   \n",
      "19381                                   The Very Best Tea   \n",
      "30724                                  Where is the Kiwi?   \n",
      "30659                                     sickening sweet   \n",
      "17672                            Lemon Medley?  Chamomile   \n",
      "160956                A truly energizing, natural elixir!   \n",
      "70582                                      good lollipops   \n",
      "120275  Best Instant Vietnamese Coffee but where is th...   \n",
      "4066               Fat free and taste free, sorry to say!   \n",
      "94767                                           Delicious   \n",
      "36093                        soy blenders, barista series   \n",
      "140402                                          Good Bran   \n",
      "215286    Like having a Starbucks right in your kitchen!!   \n",
      "2070                                 Not like the picture   \n",
      "200926                                             Tasty!   \n",
      "49848                                      Casbah Tabouli   \n",
      "39492                Tastes like regular old coffee to me   \n",
      "32608                                 Absolutely terrible   \n",
      "226965                                            Amazing   \n",
      "18417                        Made in China - and it shows   \n",
      "...                                                   ...   \n",
      "46939                                   Great Pretzels!!!   \n",
      "2669                                  sugar free cinnamon   \n",
      "224184                Impressed, they back their product!   \n",
      "35419                                          Poor Taste   \n",
      "247953            Good Stuff If You Like Angel Hair Pasta   \n",
      "295640                       Healthy Tasty chicken treats   \n",
      "274455                            The Cure For Thick Hair   \n",
      "32438                                     cocktail mistro   \n",
      "53395                                      Fruitchia bars   \n",
      "3485                                 Yep, that's a bonsai   \n",
      "2212    UPDATED: Pup loves it...but he managed to brea...   \n",
      "55443                                   Easy comfort food   \n",
      "52647                                  BAD, BAD DEAL!!!!!   \n",
      "50223                                     Tastes very bad   \n",
      "226856                Great for hair care and for cooking   \n",
      "23460                              The ultimate Green tea   \n",
      "55707                        Probably not made in the USA   \n",
      "62217                                           Tasty tea   \n",
      "56306                                           Too heavy   \n",
      "10510                          Gerber's fruits are cooked   \n",
      "21862                                       Good, but bad   \n",
      "299011                       Great Lapsong Souchong Value   \n",
      "170709                                     Great popcorn.   \n",
      "5957                                             Mediocre   \n",
      "40195            Meh.  Not that impressed. Kind of lousy.   \n",
      "301106                                   Very Nice 2-in-1   \n",
      "47858                                         Disapointed   \n",
      "56479                   Misleading and not what you think   \n",
      "48640                                        DISGUSTING!!   \n",
      "17431                                      Really bad tea   \n",
      "\n",
      "                                                     Text  \\\n",
      "21125   This is obviously a red-hot item & will defini...   \n",
      "16501   The label on  Frank's Hot Sauce says \"The Perf...   \n",
      "7834    I bought these cause I'm a chocolate snap love...   \n",
      "73503   I've tried quite a variety of K cups and this ...   \n",
      "73894   I have been drinking this coffee for the last ...   \n",
      "10235   I was in my local tea shop buying my usual Ear...   \n",
      "12061   The box shows strawberries and raspberries on ...   \n",
      "50166   I first bought this sauce when a local chief r...   \n",
      "102471  We use this on fish also. It is a great season...   \n",
      "1511    Reading the other reviews of this product is f...   \n",
      "5689    I bought these after reading all the great rev...   \n",
      "19381   I had an opportunity to try this tea at my Hai...   \n",
      "30724   I got this drink expecting the carbonated tast...   \n",
      "30659   I could not finish this drink, it is like drin...   \n",
      "17672   Selling this as Lemon Medley is a complete jok...   \n",
      "160956  I make coconut water a part of my daily regime...   \n",
      "70582   thesse suckiers really have the best ingreadie...   \n",
      "120275  I love this coffee but I can't find the plain ...   \n",
      "4066    REally wanted to like this but it has no taste...   \n",
      "94767   Delicious.  Remove paper before heating & cove...   \n",
      "36093   I thought it would have been thicker. Its actu...   \n",
      "140402  My husband eats this every day in his morning ...   \n",
      "215286  I Love this coffee--it is bold and very tasty....   \n",
      "2070    The product I reveived is just grease, and doe...   \n",
      "200926  I purchased this item out of curiosity......It...   \n",
      "49848   Easy to make and you can add beans and various...   \n",
      "39492   I agree with the previous reviewer...I don't t...   \n",
      "32608   This stuff is terrible. I am throwing the bott...   \n",
      "226965  My all time favorite treat! These cookies are ...   \n",
      "18417   I won't be buying the Roland brand again ... t...   \n",
      "...                                                   ...   \n",
      "46939   I love these pretzels!! The taste like real pr...   \n",
      "2669    I really love cinnamon so being able to eat su...   \n",
      "224184  I purchased a air popcorn popper, Presto, whic...   \n",
      "35419   I bought since it seemed like a great deal but...   \n",
      "247953  A couple of reviewers had some criticisms of t...   \n",
      "295640  Our Bichon really likes these little treats. W...   \n",
      "274455  For those of you that are tired of using a han...   \n",
      "32438   Have to disagree with the previous rating. Thi...   \n",
      "53395   The shipping on these fruit chia bars was very...   \n",
      "3485    Well not sure what to say other than I receive...   \n",
      "2212    I picked up the Buster Cube after reading some...   \n",
      "55443   Really enjoy all the Vigo rice mixes. Perfectl...   \n",
      "52647   Every single one of my twelve cans were BADLY ...   \n",
      "50223   My kids and I tried it...had to throw the whol...   \n",
      "226856  I'd suggest to buy TWO containers, one for hai...   \n",
      "23460   I LOVE this tea!!! I have been drinking it now...   \n",
      "55707   Probably not made in the USA since I can't get...   \n",
      "62217   I'm not a huge tea drinker (coffee every day) ...   \n",
      "56306   Not a product for my hair. I didn't like it at...   \n",
      "10510   I've been giving my daughter Gerber fruits, ve...   \n",
      "21862   Even after eating the just 1 serving size, I g...   \n",
      "299011  I'm by no means an expert, so this is only my ...   \n",
      "170709  I don't pretend to be a popcorn expert but eit...   \n",
      "5957    These bars are huge, with good texture and ver...   \n",
      "40195   I mean, yeah, it's coffee.  True dat.  But it'...   \n",
      "301106  My husband loves these 2-in-1 products and thi...   \n",
      "47858   We tried this formula because it is lactose fr...   \n",
      "56479   I'm always on the lookout for a great no-sugar...   \n",
      "48640   Believe it when you read all the reviews that ...   \n",
      "17431   I read the reviews, but thought how bad can th...   \n",
      "\n",
      "                                              CleanedText  \n",
      "21125   b'obvious item definit decor page appropri feb...  \n",
      "16501   b'label frank hot sauc say perfect blend flavo...  \n",
      "7834    b'bought caus chocol snap lover crave lack tra...  \n",
      "73503   b'ive tri quit varieti cup favorit prior like ...  \n",
      "73894   b'drink coffe last year drink forget pay wonde...  \n",
      "10235   b'local tea shop buy usual earl grey darjeel b...  \n",
      "12061   b'box show strawberri raspberri ingredi list d...  \n",
      "50166   b'first bought sauc local chief recommend use ...  \n",
      "102471  b'use fish also great season low cal solut gre...  \n",
      "1511    b'read review product fascin bug dust gooey me...  \n",
      "5689    b'bought read great review would advis tune bo...  \n",
      "19381   b'opportun tri tea hair salon marketspic tea u...  \n",
      "30724   b'got drink expect carbon tast combin kiwi ber...  \n",
      "30659   b'could finish drink like drink orang syrup ma...  \n",
      "17672   b'sell lemon medley complet joke look like cha...  \n",
      "160956  b'make coconut water part daili regimen whethe...  \n",
      "70582   b'thess suckier realli best ingreadi high reco...  \n",
      "120275  b'love coffe cant find plain black version any...  \n",
      "4066                b'realli want like tast stuck canist'  \n",
      "94767   b'delici remov paper heat cover slice cheddar ...  \n",
      "36093   b'thought would thicker actual like soy milk i...  \n",
      "140402  b'husband eat everi day morn cereal pleas tast...  \n",
      "215286  b'love bold tasti like starbuck home especi ni...  \n",
      "2070    b'product reveiv greas doesnt resembl product ...  \n",
      "200926  b'purchas item curios pretti good chewi like j...  \n",
      "49848   b'easi make add bean various veget hand vari d...  \n",
      "39492   b'agre previous review dont tast cinnamon dont...  \n",
      "32608   b'stuff terribl throw bottl away wasabi tast w...  \n",
      "226965  b'time favorit treat cooki amaz found local st...  \n",
      "18417   b'wont buy roland brand mackerel made china sh...  \n",
      "...                                                   ...  \n",
      "46939   b'love pretzel tast like real pretzel glutino ...  \n",
      "2669    b'realli love cinnamon abl eat sugar free cinn...  \n",
      "224184  b'purchas air popcorn popper presto work purch...  \n",
      "35419   b'bought sinc seem like great deal made batch ...  \n",
      "247953  b'coupl review critic rice product dont unders...  \n",
      "295640  b'bichon realli like littl treat use potti tre...  \n",
      "274455  b'tire use hand hair product get hair stay pla...  \n",
      "32438   b'disagre previous rate product suppos natur w...  \n",
      "53395   b'ship fruit chia bar fast wasnt free mailman ...  \n",
      "3485    b'well sure say receiv healthi look bonsai nic...  \n",
      "2212    b'pick buster cube read posit review great toy...  \n",
      "55443   b'realli enjoy vigo rice mix perfect season qu...  \n",
      "52647   b'everi singl one twelv can bad dent total was...  \n",
      "50223   b'kid tri throw whole box away sicken sweet gi...  \n",
      "226856  b'suggest buy two contain one hair use one coo...  \n",
      "23460   b'love tea drink year signific cold sinc big b...  \n",
      "55707   b'probabl made usa sinc cant get straight answ...  \n",
      "62217   b'huge tea drinker coffe everi day enjoy black...  \n",
      "56306   b'product hair didnt like matter littl use hea...  \n",
      "10510   b'ive give daughter gerber fruit veggi meat si...  \n",
      "21862   b'even eat serv size get horribl pain stomach ...  \n",
      "299011  b'mean expert person opinion lapsong souchong ...  \n",
      "170709  b'dont pretend popcorn expert either fresh eve...  \n",
      "5957    b'bar huge good textur fill tast mediocr bit f...  \n",
      "40195   b'mean yeah coffe true dat good think better e...  \n",
      "301106  b'husband love product one perform well normal...  \n",
      "47858   b'tri formula lactos free babi issu lactos int...  \n",
      "56479   b'alway lookout great sweeten current like use...  \n",
      "48640   b'believ read review say smell fishi odd textu...  \n",
      "17431      b'read review thought bad tea total tasteless'  \n",
      "\n",
      "[100 rows x 12 columns]>\n",
      "(100, 12)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from collections import Counter\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "\n",
    "#using the preprocess Sqlite Table to read the  data\n",
    "con = sqlite3.connect('./amazon-fine-food-reviews/finalsqlite')\n",
    "\n",
    "#Filtering only positive review\n",
    "positive_review = pd.read_sql_query(\"\"\"\n",
    "SELECT * FROM Reviews Where Score == 'positive'\n",
    "\"\"\", con)\n",
    "\n",
    "#Extracting 2000 Positive review Randomly\n",
    "positive_smaller_review_set = positive_review.sample(50)\n",
    "print(positive_smaller_review_set.shape)\n",
    "\n",
    "#Filtering only negative review\n",
    "negative_review = pd.read_sql_query(\"\"\"\n",
    "SELECT * FROM Reviews Where Score == 'negative'\n",
    "\"\"\", con)\n",
    "\n",
    "#Extracing 2000 random negative review\n",
    "negative_smaller_review_set = negative_review.sample(50)\n",
    "print(negative_smaller_review_set.shape)\n",
    "\n",
    "#Concanating Negative and Positive review into one dataframe\n",
    "frames = [positive_smaller_review_set, negative_smaller_review_set]\n",
    "final = pd.concat(frames)\n",
    "\n",
    "#Sort the dataframe data into the increasing Time order\n",
    "final = final.sort_values(by=['Time'])\n",
    "print(final.head)\n",
    "print(final.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to  find accuracy and optimal K(nearest Neighbor) on given approach and given vector\n",
    "\n",
    "def knn_accuracy(standarized_data , algorithms):\n",
    "    # create design matrix X and target vector y\n",
    "    X = np.array(standarized_data[:,:])  \n",
    "    y = np.array(final['Score'].values)\n",
    "\n",
    "    #Calculating the train and test size and dividing the data into train and test chunks into the 70 - 30 ratio\n",
    "    train_size = math.floor(X.shape[0] * 0.7)\n",
    "\n",
    "    #split the data set into train and test\n",
    "    X_train = X[: train_size ,:]\n",
    "    X_test  = X[train_size : , :]  \n",
    "    y_train = y[: train_size ]\n",
    "    y_test =  y[train_size : ]\n",
    "\n",
    "\n",
    "    neighbors = np.arange(1, 30 , 2)\n",
    "\n",
    "    # empty list that will hold cv scores\n",
    "    cv_scores = []\n",
    "    print(algorithms)\n",
    "     \n",
    "    # perform 10-fold cross validation\n",
    "    for k in neighbors:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k,  algorithm = algorithms )\n",
    "        scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n",
    "        cv_scores.append(scores.mean())\n",
    "\n",
    "    # changing to misclassification error\n",
    "    MSE = [1 - x for x in cv_scores]\n",
    "    print(MSE)\n",
    "    # determining best k\n",
    "    optimal_k = neighbors[MSE.index(min(MSE))]\n",
    "    print('\\nThe optimal number of neighbors is %d.' % optimal_k)\n",
    "\n",
    "\n",
    "    #calculate the accuracy on Test data\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors = optimal_k , algorithm = algorithms)\n",
    "\n",
    "    # fitting the model\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    #predict the response\n",
    "    pred = knn.predict(X_test)\n",
    "\n",
    "    # evaluate accuracy\n",
    "    acc = accuracy_score(y_test, pred) * 100\n",
    "    \n",
    "    return [optimal_k, acc]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag OF word(BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of vector of Preprocess review(text) after BOW(unigram) (100, 1737)\n"
     ]
    }
   ],
   "source": [
    "#BOW of preprocess text\n",
    "count_vect = CountVectorizer()\n",
    "final_counts_preprocess_text = count_vect.fit_transform(final['CleanedText'].values)\n",
    "\n",
    "print(\"Shape of vector of Preprocess review(text) after BOW(unigram)\",final_counts_preprocess_text.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standariztion Of Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DANAV\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Standarization OF review text\n",
    "standarized_data = StandardScaler().fit_transform(final_counts_preprocess_text.toarray()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # knn on BOW vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brute\n",
      "[0.5142857142857142, 0.5142857142857142, 0.5017857142857143, 0.5142857142857142, 0.5392857142857143, 0.5101190476190476, 0.44166666666666665, 0.4750000000000001, 0.4833333333333334, 0.475, 0.444047619047619, 0.46071428571428574, 0.4642857142857143, 0.5142857142857142, 0.5351190476190476]\n",
      "\n",
      "The optimal number of neighbors is 13.\n",
      "\n",
      "The accuracy of the knn classifier on BOW using Brute Force approach for k = 13 is 53.333333%\n",
      "kd_tree\n",
      "[0.5142857142857142, 0.5142857142857142, 0.5017857142857143, 0.5142857142857142, 0.5392857142857143, 0.5101190476190476, 0.44166666666666665, 0.4750000000000001, 0.4833333333333334, 0.475, 0.444047619047619, 0.46071428571428574, 0.4642857142857143, 0.5142857142857142, 0.5351190476190476]\n",
      "\n",
      "The optimal number of neighbors is 13.\n",
      "\n",
      "The accuracy of the knn classifier on BOW using K_D tree approach for k = 13 is 53.333333%\n"
     ]
    }
   ],
   "source": [
    "# Using knn_accuracy Function finding optimal k and accuracy on that optimal k\n",
    "\n",
    "optimal_k, acc =  knn_accuracy(standarized_data , algorithms = 'brute')\n",
    "print('\\nThe accuracy of the knn classifier on BOW using Brute Force approach for k = %d is %f%%' % (optimal_k, acc))\n",
    "optimal_k, acc =  knn_accuracy(standarized_data , algorithms = 'kd_tree')\n",
    "print('\\nThe accuracy of the knn classifier on BOW using K_D tree approach for k = %d is %f%%' % (optimal_k, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of vector of Preprocess review(text) after Tf-Idf (100, 7274)\n"
     ]
    }
   ],
   "source": [
    "#tf-idf of preprocess text\n",
    "tf_idf_vect  = TfidfVectorizer(ngram_range=(1,2))\n",
    "final_tf_idf_preprocess_text =  tf_idf_vect.fit_transform(final['Text'].values)\n",
    "\n",
    "#Shape of  vectors\n",
    "print(\"Shape of vector of Preprocess review(text) after Tf-Idf\",final_tf_idf_preprocess_text.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Standariztion Of Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Standarization OF review text\n",
    "standarized_data = StandardScaler().fit_transform(final_tf_idf_preprocess_text.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# knn on TF-IDF vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brute\n",
      "[0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5267857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.4708333333333333, 0.5, 0.5125, 0.4976190476190475, 0.4976190476190476]\n",
      "\n",
      "The optimal number of neighbors is 21.\n",
      "\n",
      "The accuracy of the knn classifier on Tf-Idf using Brute Force approach for k = 21 is 53.333333%\n",
      "kd_tree\n",
      "[0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5267857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.4708333333333333, 0.5, 0.5125, 0.4976190476190475, 0.4976190476190476]\n",
      "\n",
      "The optimal number of neighbors is 21.\n",
      "\n",
      "The accuracy of the knn classifier on Tf-Idf using K_D tree approach for k = 21 is 53.333333%\n"
     ]
    }
   ],
   "source": [
    "# Using knn_accuracy Function finding optimal k and accuracy on that optimal k\n",
    "\n",
    "optimal_k, acc =  knn_accuracy(standarized_data , algorithms = 'brute')\n",
    "print('\\nThe accuracy of the knn classifier on Tf-Idf using Brute Force approach for k = %d is %f%%' % (optimal_k, acc))\n",
    "optimal_k, acc =  knn_accuracy(standarized_data , algorithms = 'kd_tree')\n",
    "print('\\nThe accuracy of the knn classifier on Tf-Idf using K_D tree approach for k = %d is %f%%' % (optimal_k, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition oF CleanHtml and CleanPunc for Preprocessing the reviw text\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "def cleanhtml(sentence): #function to clean the word of any html-tags\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', sentence)\n",
    "    return cleantext\n",
    "def cleanpunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    return  cleaned\n",
    "\n",
    "# Train your own Word2Vec model using your own text corpus\n",
    "\n",
    "i=0\n",
    "list_of_sent=[]\n",
    "for sent in final['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "    sent=cleanhtml(sent)\n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if(cleaned_words.isalpha()):    \n",
    "                filtered_sentence.append(cleaned_words.lower())\n",
    "            else:\n",
    "                continue \n",
    "    list_of_sent.append(filtered_sentence)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 50)\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "w2v_model=gensim.models.Word2Vec(list_of_sent,min_count=1,size=50, workers=4)\n",
    "                                 \n",
    "Avg_sent_vectors = []\n",
    "for sent in list_of_sent:\n",
    "    sent_vect = np.zeros(50)\n",
    "    count = 0\n",
    "    for word in sent:\n",
    "        try :\n",
    "            sent_vect += w2v_model.wv[word]\n",
    "            count = count + 1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    sent_vect = sent_vect / count\n",
    "    Avg_sent_vectors.append(sent_vect)\n",
    "     \n",
    "#Standarization Of Avg Word2Vec vector\n",
    "standardized_data = StandardScaler().fit_transform(Avg_sent_vectors)\n",
    "print(standardized_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# knn on Average Word2Vec vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brute\n",
      "[0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5267857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.4708333333333333, 0.5, 0.5125, 0.4976190476190475, 0.4976190476190476]\n",
      "\n",
      "The optimal number of neighbors is 21.\n",
      "\n",
      "The accuracy of the knn classifier on Word2vec using Brute Force approach for k = 21 is 53.333333%\n",
      "kd_tree\n",
      "[0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5267857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.4708333333333333, 0.5, 0.5125, 0.4976190476190475, 0.4976190476190476]\n",
      "\n",
      "The optimal number of neighbors is 21.\n",
      "\n",
      "The accuracy of the knn classifier on Word2vec using K_D tree approach for k = 21 is 53.333333%\n"
     ]
    }
   ],
   "source": [
    "# Using knn_accuracy Function finding optimal k and accuracy on that optimal k\n",
    "\n",
    "optimal_k, acc =  knn_accuracy(standarized_data , algorithms = 'brute')\n",
    "print('\\nThe accuracy of the knn classifier on Word2vec using Brute Force approach for k = %d is %f%%' % (optimal_k, acc))\n",
    "optimal_k, acc =  knn_accuracy(standarized_data , algorithms = 'kd_tree')\n",
    "print('\\nThe accuracy of the knn classifier on Word2vec using K_D tree approach for k = %d is %f%%' % (optimal_k, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-Idf Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DANAV\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 50)\n"
     ]
    }
   ],
   "source": [
    "tf_idf_feat = tf_idf_vect.get_feature_names()\n",
    "tfidf_sent_vect=[]\n",
    "row= 0;\n",
    "for sent in list_of_sent:\n",
    "    sent_vec = np.zeros(50)\n",
    "    weight_sum = 0\n",
    "    for word in sent:\n",
    "        try:\n",
    "            if((word in  w2v_model.wv.vocab) and (word in tf_idf_feat)):\n",
    "                w_vec = w2v_model.wv[word]\n",
    "                tf_idf_vec = final_tf_idf[row,tf_idf_feat.index(word)]\n",
    "                sent_vec = sent_vec + (w_vec*tf_idf_vec)\n",
    "                weight_sum = weight_sum + tf_idf_vec\n",
    "        except:\n",
    "            pass\n",
    "    sent_vec = sent_vec/weight_sum\n",
    "    tfidf_sent_vect.append(sent_vec)\n",
    "    row= row+1\n",
    "    \n",
    "# Removing NAN values    \n",
    "tfidf_sent_vect_values = np.nan_to_num(tfidf_sent_vect)\n",
    "\n",
    "#Standarization Of Tf-Idf Word2Vec Vector\n",
    "standardized_data = StandardScaler().fit_transform(tfidf_sent_vect_values)\n",
    "print(standardized_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# knn on Tf-Idf Word2Vec vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brute\n",
      "[0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5267857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.4708333333333333, 0.5, 0.5125, 0.4976190476190475, 0.4976190476190476]\n",
      "\n",
      "The optimal number of neighbors is 21.\n",
      "\n",
      "The accuracy of the knn classifier on Tf-Idf Word2vec using Brute Force approach for k = 21 is 53.333333%\n",
      "kd_tree\n",
      "[0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5267857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.5142857142857142, 0.4708333333333333, 0.5, 0.5125, 0.4976190476190475, 0.4976190476190476]\n",
      "\n",
      "The optimal number of neighbors is 21.\n",
      "\n",
      "The accuracy of the knn classifier on Tf-Idf Word2vec using K_D tree approach for k = 21 is 53.333333%\n"
     ]
    }
   ],
   "source": [
    "# Using knn_accuracy Function finding optimal k and accuracy on that optimal k\n",
    "\n",
    "optimal_k, acc =  knn_accuracy(standarized_data , algorithms = 'brute')\n",
    "print('\\nThe accuracy of the knn classifier on Tf-Idf Word2vec using Brute Force approach for k = %d is %f%%' % (optimal_k, acc))\n",
    "optimal_k, acc =  knn_accuracy(standarized_data , algorithms = 'kd_tree')\n",
    "print('\\nThe accuracy of the knn classifier on Tf-Idf Word2vec using K_D tree approach for k = %d is %f%%' % (optimal_k, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
