{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Keras -- MLPs on MNIST"},{"metadata":{"trusted":true},"cell_type":"code","source":"# if you keras is not using tensorflow as backend set \"KERAS_BACKEND=tensorflow\" use this command\n\nfrom keras.utils import np_utils\nfrom keras.datasets import mnist\nimport seaborn as sns\nfrom keras.initializers import RandomNormal","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib notebook\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\n# https://gist.github.com/greydanus/f6eee59eaf1d90fcb3b534a25362cea4\n# https://stackoverflow.com/a/14434334\n# this function is used to update the plots for each epoch and error\ndef plt_dynamic(x, vy, ty, ax, colors=['b']):\n    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n    ax.plot(x, ty, 'r', label=\"Train Loss\")\n    plt.legend()\n    plt.grid()\n    fig.canvas.draw()","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the data, shuffled and split between train and test sets \n(X_train,y_train), (X_test, y_test) = mnist.load_data()","execution_count":3,"outputs":[{"output_type":"stream","text":"Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n11493376/11490434 [==============================] - 1s 0us/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of training examples :\", X_train.shape[0], \"and each image is of shape (%d, %d)\"%(X_train.shape[1], X_train.shape[2]))\nprint(\"Number of test examples :\", X_test.shape[0], \"and each image is of shape (%d, %d)\"%(X_test.shape[1], X_test.shape[2]))","execution_count":4,"outputs":[{"output_type":"stream","text":"Number of training examples : 60000 and each image is of shape (28, 28)\nNumber of test examples : 10000 and each image is of shape (28, 28)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# if you observe the input shape its 3 dimensional vector\n# for each image we have a (28*28) vector\n# we will convert the (28*28) vector into single dimensional vector of 1 * 784 \n\nX_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2]) \nX_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2]) ","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# after converting the input images from 3d to 2d vectors\n\nprint(\"Number of training examples :\", X_train.shape[0], \"and each image is of shape (%d)\"%(X_train.shape[1]))\nprint(\"Number of training examples :\", X_test.shape[0], \"and each image is of shape (%d)\"%(X_test.shape[1]))","execution_count":6,"outputs":[{"output_type":"stream","text":"Number of training examples : 60000 and each image is of shape (784)\nNumber of training examples : 10000 and each image is of shape (784)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# An example data point\nprint(X_train[0])","execution_count":7,"outputs":[{"output_type":"stream","text":"[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   3  18  18  18 126 136 175  26 166 255\n 247 127   0   0   0   0   0   0   0   0   0   0   0   0  30  36  94 154\n 170 253 253 253 253 253 225 172 253 242 195  64   0   0   0   0   0   0\n   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251  93  82\n  82  56  39   0   0   0   0   0   0   0   0   0   0   0   0  18 219 253\n 253 253 253 253 198 182 247 241   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0  14   1 154 253  90   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0  11 190 253  70   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  35 241\n 225 160 108   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0  81 240 253 253 119  25   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0  45 186 253 253 150  27   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252 253 187\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0 249 253 249  64   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n 253 207   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0  39 148 229 253 253 253 250 182   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253\n 253 201  78   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0  23  66 213 253 253 253 253 198  81   2   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0  18 171 219 253 253 253 253 195\n  80   9   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n  55 172 226 253 253 253 253 244 133  11   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0 136 253 253 253 212 135 132  16\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# if we observe the above matrix each cell is having a value between 0-255\n# before we move to apply machine learning algorithms lets try to normalize the data\n# X => (X - Xmin)/(Xmax-Xmin) = X/255\n\nX_train = X_train/255\nX_test = X_test/255","execution_count":8,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# example data point after normlizing\nprint(X_train[0])","execution_count":9,"outputs":[{"output_type":"stream","text":"[0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n 0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.\n 0.96862745 0.49803922 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.19215686\n 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n 0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863\n 0.32156863 0.21960784 0.15294118 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.07058824 0.85882353 0.99215686\n 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549\n 0.96862745 0.94509804 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n 0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.04313725\n 0.74509804 0.99215686 0.2745098  0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.1372549  0.94509804\n 0.88235294 0.62745098 0.42352941 0.00392157 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.31764706 0.94117647 0.99215686\n 0.99215686 0.46666667 0.09803922 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n 0.58823529 0.10588235 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.97647059 0.99215686 0.97647059 0.25098039 0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n 0.99215686 0.81176471 0.00784314 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.15294118 0.58039216\n 0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686\n 0.99215686 0.78823529 0.30588235 0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.07058824 0.67058824\n 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588\n 0.31372549 0.03529412 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686\n 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.53333333 0.99215686\n 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.        ]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here we are having a class number for each image\nprint(\"Class label of first image :\", y_train[0])\n\n# lets convert this into a 10 dimensional vector\n# ex: consider an image is 5 convert it into 5 => [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n# this conversion needed for MLPs \n\nY_train = np_utils.to_categorical(y_train, 10)\nY_test = np_utils.to_categorical(y_test, 10)\nprint(\"After converting the output into a vector : \",Y_train[0])","execution_count":10,"outputs":[{"output_type":"stream","text":"Class label of first image : 5\nAfter converting the output into a vector :  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<h2>  Softmax classifier  </h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://keras.io/getting-started/sequential-model-guide/\n\n# The Sequential model is a linear stack of layers.\n# you can create a Sequential model by passing a list of layer instances to the constructor:\n\n# model = Sequential([\n#     Dense(32, input_shape=(784,)),\n#     Activation('relu'),\n#     Dense(10),\n#     Activation('softmax'),\n# ])\n\n# You can also simply add layers via the .add() method:\n\n# model = Sequential()\n# model.add(Dense(32, input_dim=784))\n# model.add(Activation('relu'))\n\n###\n\n# https://keras.io/layers/core/\n\n# keras.layers.Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', \n# bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, \n# kernel_constraint=None, bias_constraint=None)\n\n# Dense implements the operation: output = activation(dot(input, kernel) + bias) where\n# activation is the element-wise activation function passed as the activation argument, \n# kernel is a weights matrix created by the layer, and \n# bias is a bias vector created by the layer (only applicable if use_bias is True).\n\n# output = activation(dot(input, kernel) + bias)  => y = activation(WT. X + b)\n\n####\n\n# https://keras.io/activations/\n\n# Activations can either be used through an Activation layer, or through the activation argument supported by all forward layers:\n\n# from keras.layers import Activation, Dense\n\n# model.add(Dense(64))\n# model.add(Activation('tanh'))\n\n# This is equivalent to:\n# model.add(Dense(64, activation='tanh'))\n\n# there are many activation functions ar available ex: tanh, relu, softmax\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# some model parameters\n\noutput_dim = 10\ninput_dim = X_train.shape[1]\n\nbatch_size = 128\nnb_epoch = 20","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# start building a model\nmodel = Sequential()\n\n# The model needs to know what input shape it should expect. \n# For this reason, the first layer in a Sequential model \n# (and only the first, because following layers can do automatic shape inference)\n# needs to receive information about its input shape. \n# you can use input_shape and input_dim to pass the shape of input\n\n# output_dim represent the number of nodes need in that layer\n# here we have 10 nodes\n\nmodel.add(Dense(output_dim, input_dim = input_dim , activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Before training a model, you need to configure the learning process, which is done via the compile method\n\n# It receives three arguments:\n# An optimizer. This could be the string identifier of an existing optimizer , https://keras.io/optimizers/\n# A loss function. This is the objective that the model will try to minimize., https://keras.io/losses/\n# A list of metrics. For any classification problem you will want to set this to metrics=['accuracy'].  https://keras.io/metrics/\n\n\n# Note: when using the categorical_crossentropy loss, your targets should be in categorical format \n# (e.g. if you have 10 classes, the target for each sample should be a 10-dimensional vector that is all-zeros except \n# for a 1 at the index corresponding to the class of the sample).\n\n# that is why we converted out labels into vectors\n\nmodel.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Keras models are trained on Numpy arrays of input data and labels. \n# For training a model, you will typically use the  fit function\n\n# fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, \n# validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, \n# validation_steps=None)\n\n# fit() function Trains the model for a fixed number of epochs (iterations on a dataset).\n\n# it returns A History object. Its History.history attribute is a record of training loss values and \n# metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).\n\n# https://github.com/openai/baselines/issues/20\n\nhistory = model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test)) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\n# print(history.history.keys())\n# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n\n# we will get val_loss and val_acc only when you pass the paramter validation_data\n# val_loss : validation loss\n# val_acc : validation accuracy\n\n# loss : training loss\n# acc : train accuracy\n# for each key in histrory.histrory we will have a list of length equal to number of epochs\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" <h3>  MLP + Sigmoid activation + SGDOptimizer </h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Multilayer perceptron\n\nmodel_sigmoid = Sequential()\nmodel_sigmoid.add(Dense(512, activation = 'sigmoid', input_shape = (input_dim,)))\nmodel_sigmoid.add(Dense(128, activation = 'sigmoid'))\nmodel_sigmoid.add(Dense(output_dim, activation='softmax'))\n\nmodel_sigmoid.summary()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"model_sigmoid.compile(optimizer='sgd', loss= 'categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_sigmoid.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=2, validation_data=(X_test, Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model_sigmoid.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\n# print(history.history.keys())\n# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n\n# we will get val_loss and val_acc only when you pass the paramter validation_data\n# val_loss : validation loss\n# val_acc : validation accuracy\n\n# loss : training loss\n# acc : train accuracy\n# for each key in histrory.histrory we will have a list of length equal to number of epochs\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w_after = model_sigmoid.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure()\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>MLP + Sigmoid activation + ADAM </h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_sigmoid = Sequential()\nmodel_sigmoid.add(Dense(512, activation='sigmoid', input_shape=(input_dim,)))\nmodel_sigmoid.add(Dense(128, activation='sigmoid'))\nmodel_sigmoid.add(Dense(output_dim, activation='softmax'))\n\nmodel_sigmoid.summary()\n\nmodel_sigmoid.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_sigmoid.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model_sigmoid.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\n# print(history.history.keys())\n# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n\n# we will get val_loss and val_acc only when you pass the paramter validation_data\n# val_loss : validation loss\n# val_acc : validation accuracy\n\n# loss : training loss\n# acc : train accuracy\n# for each key in histrory.histrory we will have a list of length equal to number of epochs\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w_after = model_sigmoid.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure()\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> MLP + ReLU +SGD </h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Multilayer perceptron\n\n# https://arxiv.org/pdf/1707.09725.pdf#page=95\n# for relu layers\n# If we sample weights from a normal distribution N(0,σ) we satisfy this condition with σ=√(2/(ni). \n# h1 =>  σ=√(2/(fan_in) = 0.062  => N(0,σ) = N(0,0.062)\n# h2 =>  σ=√(2/(fan_in) = 0.125  => N(0,σ) = N(0,0.125)\n# out =>  σ=√(2/(fan_in+1) = 0.120  => N(0,σ) = N(0,0.120)\n\nmodel_relu = Sequential()\nmodel_relu.add(Dense(512, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_relu.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\nmodel_relu.add(Dense(output_dim, activation='softmax'))\n\nmodel_relu.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"model_relu.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_relu.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model_relu.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\n# print(history.history.keys())\n# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n\n# we will get val_loss and val_acc only when you pass the paramter validation_data\n# val_loss : validation loss\n# val_acc : validation accuracy\n\n# loss : training loss\n# acc : train accuracy\n# for each key in histrory.histrory we will have a list of length equal to number of epochs\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w_after = model_relu.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure()\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> MLP + ReLU + ADAM </h2>"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"model_relu = Sequential()\nmodel_relu.add(Dense(512, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_relu.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\nmodel_relu.add(Dense(output_dim, activation='softmax'))\n\nprint(model_relu.summary())\n\nmodel_relu.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_relu.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model_relu.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\n# print(history.history.keys())\n# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n\n# we will get val_loss and val_acc only when you pass the paramter validation_data\n# val_loss : validation loss\n# val_acc : validation accuracy\n\n# loss : training loss\n# acc : train accuracy\n# for each key in histrory.histrory we will have a list of length equal to number of epochs\n\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w_after = model_relu.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure()\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> MLP + Batch-Norm on hidden Layers + AdamOptimizer </2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Multilayer perceptron\n\n# https://intoli.com/blog/neural-network-initialization/ \n# If we sample weights from a normal distribution N(0,σ) we satisfy this condition with σ=√(2/(ni+ni+1). \n# h1 =>  σ=√(2/(ni+ni+1) = 0.039  => N(0,σ) = N(0,0.039)\n# h2 =>  σ=√(2/(ni+ni+1) = 0.055  => N(0,σ) = N(0,0.055)\n# h1 =>  σ=√(2/(ni+ni+1) = 0.120  => N(0,σ) = N(0,0.120)\n\nfrom keras.layers.normalization import BatchNormalization\n\nmodel_batch = Sequential()\n\nmodel_batch.add(Dense(512, activation='sigmoid', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_batch.add(BatchNormalization())\n\nmodel_batch.add(Dense(128, activation='sigmoid', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_batch.add(BatchNormalization())\n\nmodel_batch.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_batch.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_batch.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_batch.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model_batch.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\n# print(history.history.keys())\n# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n\n# we will get val_loss and val_acc only when you pass the paramter validation_data\n# val_loss : validation loss\n# val_acc : validation accuracy\n\n# loss : training loss\n# acc : train accuracy\n# for each key in histrory.histrory we will have a list of length equal to number of epochs\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w_after = model_batch.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure()\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> 5. MLP + Dropout + AdamOptimizer </h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://stackoverflow.com/questions/34716454/where-do-i-call-the-batchnormalization-function-in-keras\n\nfrom keras.layers import Dropout\n\nmodel_drop = Sequential()\n\nmodel_drop.add(Dense(512, activation='sigmoid', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_drop.add(BatchNormalization())\nmodel_drop.add(Dropout(0.5))\n\nmodel_drop.add(Dense(128, activation='sigmoid', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_drop.add(BatchNormalization())\nmodel_drop.add(Dropout(0.5))\n\nmodel_drop.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_drop.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_drop.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model_drop.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\n# print(history.history.keys())\n# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n\n# we will get val_loss and val_acc only when you pass the paramter validation_data\n# val_loss : validation loss\n# val_acc : validation accuracy\n\n# loss : training loss\n# acc : train accuracy\n# for each key in histrory.histrory we will have a list of length equal to number of epochs\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w_after = model_drop.get_weights()\n\nh1_w = w_after[0].flatten().reshape(-1,1)\nh2_w = w_after[2].flatten().reshape(-1,1)\nout_w = w_after[4].flatten().reshape(-1,1)\n\n\nfig = plt.figure()\nplt.title(\"Weight matrices after model trained\")\nplt.subplot(1, 3, 1)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h1_w,color='b')\nplt.xlabel('Hidden Layer 1')\n\nplt.subplot(1, 3, 2)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=h2_w, color='r')\nplt.xlabel('Hidden Layer 2 ')\n\nplt.subplot(1, 3, 3)\nplt.title(\"Trained model Weights\")\nax = sns.violinplot(y=out_w,color='y')\nplt.xlabel('Output Layer ')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2> Hyper-parameter tuning of Keras models using Sklearn </h2>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.optimizers import Adam,RMSprop,SGD\ndef best_hyperparameters(activ):\n\n    model = Sequential()\n    model.add(Dense(512, activation=activ, input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\n    model.add(Dense(128, activation=activ, kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\n    model.add(Dense(output_dim, activation='softmax'))\n\n\n    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n\nactiv = ['sigmoid','relu']\n\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nmodel = KerasClassifier(build_fn=best_hyperparameters, epochs=nb_epoch, batch_size=batch_size, verbose=0)\nparam_grid = dict(activ=activ)\n\n# if you are using CPU\n# grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n# if you are using GPU dont use the n_jobs parameter\n\ngrid = GridSearchCV(estimator=model, param_grid=param_grid)\ngrid_result = grid.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Keras with Two Hidden Layer**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom keras.initializers import TruncatedNormal\n\nmodel_one = Sequential()\n\n#Dense Layer with 256 units\nmodel_one.add(Dense(256, activation='relu', input_shape=(input_dim,), kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05, seed=None)))\nmodel_one.add(BatchNormalization())\nmodel_one.add(Dropout(0.5))\n\n#Dense Layer with 256 units\nmodel_one.add(Dense(128, activation='relu', kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05, seed=None)))\nmodel_one.add(BatchNormalization())\nmodel_one.add(Dropout(0.5))\n\n#Output layer\nmodel_one.add(Dense(output_dim, activation='softmax'))\n\n#Summary of the model\nmodel_one.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Compiling and Training the model\nmodel_one.compile(optimizer='adam', loss='categorical_crossentropy' , metrics=['accuracy'])\nhistory= model_one.fit(X_train, Y_train, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model_one.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Keras Model with three Hidden layer**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_two =  Sequential()\n\n#Dense layer with 512 Units\nmodel_two.add(Dense(512, activation='relu' ,input_shape=(input_dim,), use_bias=True , bias_initializer='zeros', kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05, seed=None)))\nmodel_two.add(BatchNormalization())\nmodel_two.add(Dropout(0.5))\n\n#Dense layer with 256 Units\nmodel_two.add(Dense(256, activation='relu', use_bias=True , bias_initializer='zeros', kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05, seed=None)))\nmodel_two.add(BatchNormalization())\nmodel_two.add(Dropout(0.5))\n\n#Dense layer with 128 Units\nmodel_two.add(Dense(128, activation='relu', use_bias=True , bias_initializer='zeros', kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05, seed=None)))\nmodel_two.add(BatchNormalization())\nmodel_two.add(Dropout(0.5))\n\n#Output layer\nmodel_two.add(Dense(output_dim, activation='softmax'))\n\n#Summary\nmodel_two.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Compile and Fitting of the model\nmodel_two.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model_two.fit(X_train, Y_train, epochs=nb_epoch, verbose=1 , validation_data=(X_test, Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting Train and Validation loss\nscore = model_two.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Keras Model with Five layer**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_three =  Sequential()\n\n#Dense layer with 512 Units\nmodel_three.add(Dense(512, activation='relu' ,input_shape=(input_dim,), use_bias=True , bias_initializer='zeros', kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05, seed=None)))\nmodel_three.add(BatchNormalization())\nmodel_three.add(Dropout(0.8))\n\n#Dense layer with 256 Units\nmodel_three.add(Dense(256, activation='relu', use_bias=True , bias_initializer='zeros', kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05, seed=None)))\nmodel_three.add(BatchNormalization())\nmodel_three.add(Dropout(0.8))\n\n#Dense layer with 128 Units\nmodel_three.add(Dense(128, activation='relu', use_bias=True , bias_initializer='zeros', kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05, seed=None)))\nmodel_three.add(BatchNormalization())\nmodel_three.add(Dropout(0.8))\n\n#Dense layer with 64 Units\nmodel_three.add(Dense(64, activation='relu', use_bias=True , bias_initializer='zeros', kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05, seed=None)))\nmodel_three.add(BatchNormalization())\nmodel_three.add(Dropout(0.8))\n\n#Dense layer with 32 Units\nmodel_three.add(Dense(32, activation='relu', use_bias=True , bias_initializer='zeros', kernel_initializer=TruncatedNormal(mean=0.0, stddev=0.05, seed=None)))\nmodel_three.add(BatchNormalization())\nmodel_three.add(Dropout(0.8))\n\n#Output layer\nmodel_three.add(Dense(output_dim, activation='softmax'))\n\n#Summary\nmodel_two.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plotting Train and Validation loss\nscore = model_two.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.1"}},"nbformat":4,"nbformat_minor":1}